warning = FALSE,
fig.width = 15,
fig.height = 7,
figure.align = "center",
cache = TRUE,
cache.lazy = FALSE
)
#knitr::opts_knit$set(root.dir = "C:\\Users\\Tianl\\Documents\\GitHub\\First-Street-Foundation")
options(htmltools.dir.version = FALSE)
h2o.init()
data <- h2o.importFile("C:/Users/Administrator/Documents/GitHub/First-Street-Foundation/dta_processed/home_dta_completed.csv")
h2o.describe(data)
splits <- h2o.splitFrame(data = data,
ratios = c(0.7, 0.15),  # partition data into 70%, 15%, 15% chunks
destination_frames = c("train", "valid", "test"), # frame ID (not required)
seed = 1)  # setting a seed will guarantee reproducibility
train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]
myY <- "deedlastsaleprice"
myX <- setdiff(names(data), c(myY,"C1"))  #remove the interest rate column because it's correlated with the outcome
print(myX)
# Run GLM
glm <- h2o.glm(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
family            = "poisson",
standardize = TRUE
)
h2o.varimp_plot(glm)
glm_perf <- h2o.performance(model = glm,
newdata = test)
print(glm_perf)
gbm <- h2o.gbm(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
ntrees            = 500,
max_depth         = 6,
learn_rate        = 0.1
)
h2o.varimp_plot(gbm)
gbm_perf <- h2o.performance(model = gbm,
newdata = test)
print(gbm_perf)
gbm_params <- list(
learn_rate = c(0.01, 0.1),
max_depth = c(5, 10, 20),
sample_rate = c(0.8, 1.0),
col_sample_rate = c(0.2, 0.5, 1.0)
)
gbm_grid <- h2o.grid(
"gbm",
x = myX,
y = myY,
grid_id = "gbm_grid",
training_frame = train,
validation_frame = valid,
ntrees = 100,
seed = 1,
hyper_params = gbm_params
)
gbm_gridperf <- h2o.getGrid(grid_id = "gbm_grid",
sort_by = "mae",
decreasing = FALSE)
print(gbm_gridperf)
best_gbm <- h2o.getModel(gbm_gridperf@model_ids[[1]])
best_gbm_perf <- h2o.performance(model = best_gbm,
newdata = test)
print(best_gbm_perf)
drf <- h2o.randomForest(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
ntrees            = 100,
max_depth         = 10
)
h2o.varimp_plot(drf)
drf_perf <- h2o.performance(model = drf,
newdata = test)
print(drf_perf)
drf_params <- list(
max_depth = c(5, 10, 20),
sample_rate = c(0.8, 1.0)
)
drf_grid <- h2o.grid(
"drf",
x = myX,
y = myY,
grid_id = "drf_grid",
training_frame = train,
validation_frame = valid,
ntrees = 100,
seed = 1,
hyper_params = drf_params
)
drf_gridperf <- h2o.getGrid(grid_id = "drf_grid",
sort_by = "mae",
decreasing = FALSE)
print(drf_gridperf)
best_drf <- h2o.getModel(drf_gridperf@model_ids[[1]])
best_drf_perf <- h2o.performance(model = best_drf,
newdata = test)
print(best_drf_perf)
dl <- h2o.deeplearning(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test
)
h2o.varimp_plot(dl)
dl_perf <- h2o.performance(model = dl,
newdata = test)
print(dl_perf)
dl_params <- list(
activation = c(
"Rectifier",
"Maxout",
"Tanh",
"RectifierWithDropout",
"MaxoutWithDropout",
"TanhWithDropout"
),
hidden = list(c(5, 5, 5, 5, 5), c(10, 10, 10, 10), c(50, 50, 50), c(100, 100, 100)),
epochs = c(50, 100, 200)
#l1 = c(0, 0.00001, 0.0001),
#l2 = c(0, 0.00001, 0.0001),
#rate = c(0, 01, 0.005, 0.001),
#rate_annealing = c(1e-8, 1e-7, 1e-6),
#rho = c(0.9, 0.95, 0.99, 0.999),
#epsilon = c(1e-10, 1e-8, 1e-6, 1e-4),
#momentum_start = c(0, 0.5),
#momentum_stable = c(0.99, 0.5, 0),
#input_dropout_ratio = c(0, 0.1, 0.2),
#max_w2 = c(10, 100, 1000, 3.4028235e+38)
)
dl_grid <- h2o.grid(
"deeplearning",
x = myX,
y = myY,
grid_id = "dl_grid",
training_frame = train,
validation_frame = valid,
seed = 1,
hyper_params = dl_params
)
dl_gridperf <- h2o.getGrid(grid_id = "dl_grid",
sort_by = "mae",
decreasing = FALSE)
print(dl_gridperf)
best_dl <- h2o.getModel(dl_gridperf@model_ids[[1]])
best_dl_perf <- h2o.performance(model = best_dl,
newdata = test)
print(best_dl_perf)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
vavalidation_frame = test,
base_models = list(best_gbm,
best_drf,
best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(best_gbm,
best_drf,
best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
vavalidation_frame = test,
base_models = list(best_gbm,
best_drf,
best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(best_gbm,
best_drf,
best_dl))
?h2o.stackedEnsemble
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(#best_gbm,
best_drf,
best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(#best_gbm,
#best_drf,
best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(#best_gbm,
best_drf,
#best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(#best_gbm,
best_drf,
#best_dl)
)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(best_gbm,
best_drf,
#best_dl)
)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(best_gbm,
best_drf
#best_dl)
)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(best_gbm,
best_drf)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(best_gbm,
best_drf)
)
# Run GLM
glm <- h2o.glm(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
keep_cross_validation_predictions=True
family            = "poisson",
# Run GLM
glm <- h2o.glm(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
keep_cross_validation_predictions=TRUE,
family            = "poisson",
standardize = TRUE
)
gbm <- h2o.gbm(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
keep_cross_validation_predictions=TRUE,
ntrees            = 500,
max_depth         = 6,
learn_rate        = 0.1
)
# Load packages.
packages <- c("tidyverse",
"data.table",
"h2o"
)
packages <- lapply(packages, FUN = function(x) {
if(!require(x, character.only = TRUE)) {
install.packages(x)
library(x, character.only = TRUE)
}
}
)
library("knitr")
knitr::opts_chunk$set(
echo = FALSE,
eval = TRUE,
message = FALSE,
warning = FALSE,
fig.width = 15,
fig.height = 7,
figure.align = "center",
cache = TRUE,
cache.lazy = FALSE
)
#knitr::opts_knit$set(root.dir = "C:\\Users\\Tianl\\Documents\\GitHub\\First-Street-Foundation")
options(htmltools.dir.version = FALSE)
h2o.init()
data <- h2o.importFile("C:/Users/Administrator/Documents/GitHub/First-Street-Foundation/dta_processed/home_dta_completed.csv")
h2o.describe(data)
splits <- h2o.splitFrame(data = data,
ratios = c(0.7, 0.15),  # partition data into 70%, 15%, 15% chunks
destination_frames = c("train", "valid", "test"), # frame ID (not required)
seed = 1)  # setting a seed will guarantee reproducibility
train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]
myY <- "deedlastsaleprice"
myX <- setdiff(names(data), c(myY,"C1"))  #remove the interest rate column because it's correlated with the outcome
print(myX)
# Run GLM
glm <- h2o.glm(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
keep_cross_validation_predictions=TRUE,
family            = "poisson",
standardize = TRUE
)
h2o.varimp_plot(glm)
glm_perf <- h2o.performance(model = glm,
newdata = test)
print(glm_perf)
gbm <- h2o.gbm(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
keep_cross_validation_predictions=TRUE,
ntrees            = 500,
max_depth         = 6,
learn_rate        = 0.1
)
h2o.varimp_plot(gbm)
gbm_perf <- h2o.performance(model = gbm,
newdata = test)
print(gbm_perf)
gbm_params <- list(
learn_rate = c(0.01, 0.1),
max_depth = c(5, 10, 20),
sample_rate = c(0.8, 1.0),
col_sample_rate = c(0.2, 0.5, 1.0)
)
gbm_grid <- h2o.grid(
"gbm",
x = myX,
y = myY,
grid_id = "gbm_grid",
training_frame = train,
validation_frame = valid,
keep_cross_validation_predictions=TRUE,
ntrees = 100,
seed = 1,
hyper_params = gbm_params
)
gbm_gridperf <- h2o.getGrid(grid_id = "gbm_grid",
sort_by = "mae",
decreasing = FALSE)
print(gbm_gridperf)
best_gbm <- h2o.getModel(gbm_gridperf@model_ids[[1]])
best_gbm_perf <- h2o.performance(model = best_gbm,
newdata = test)
print(best_gbm_perf)
drf <- h2o.randomForest(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
keep_cross_validation_predictions=TRUE,
ntrees            = 100,
max_depth         = 10
)
h2o.varimp_plot(drf)
drf_perf <- h2o.performance(model = drf,
newdata = test)
print(drf_perf)
drf_params <- list(
max_depth = c(5, 10, 20),
sample_rate = c(0.8, 1.0)
)
drf_grid <- h2o.grid(
"drf",
x = myX,
y = myY,
grid_id = "drf_grid",
training_frame = train,
validation_frame = valid,
keep_cross_validation_predictions=TRUE,
ntrees = 100,
seed = 1,
hyper_params = drf_params
)
drf_gridperf <- h2o.getGrid(grid_id = "drf_grid",
sort_by = "mae",
decreasing = FALSE)
print(drf_gridperf)
best_drf <- h2o.getModel(drf_gridperf@model_ids[[1]])
best_drf_perf <- h2o.performance(model = best_drf,
newdata = test)
print(best_drf_perf)
dl <- h2o.deeplearning(
x = myX,
y = myY,
training_frame    = train,
validation_frame  = test,
keep_cross_validation_predictions=TRUE,
)
h2o.varimp_plot(dl)
dl_perf <- h2o.performance(model = dl,
newdata = test)
print(dl_perf)
dl_params <- list(
activation = c(
"Rectifier",
"Maxout",
"Tanh",
"RectifierWithDropout",
"MaxoutWithDropout",
"TanhWithDropout"
),
hidden = list(c(5, 5, 5, 5, 5), c(10, 10, 10, 10), c(50, 50, 50), c(100, 100, 100)),
epochs = c(50, 100, 200)
#l1 = c(0, 0.00001, 0.0001),
#l2 = c(0, 0.00001, 0.0001),
#rate = c(0, 01, 0.005, 0.001),
#rate_annealing = c(1e-8, 1e-7, 1e-6),
#rho = c(0.9, 0.95, 0.99, 0.999),
#epsilon = c(1e-10, 1e-8, 1e-6, 1e-4),
#momentum_start = c(0, 0.5),
#momentum_stable = c(0.99, 0.5, 0),
#input_dropout_ratio = c(0, 0.1, 0.2),
#max_w2 = c(10, 100, 1000, 3.4028235e+38)
)
dl_grid <- h2o.grid(
"deeplearning",
x = myX,
y = myY,
grid_id = "dl_grid",
training_frame = train,
validation_frame = valid,
keep_cross_validation_predictions=TRUE,
seed = 1,
hyper_params = dl_params
)
dl_gridperf <- h2o.getGrid(grid_id = "dl_grid",
sort_by = "mae",
decreasing = FALSE)
print(dl_gridperf)
best_dl <- h2o.getModel(dl_gridperf@model_ids[[1]])
best_dl_perf <- h2o.performance(model = best_dl,
newdata = test)
print(best_dl_perf)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
keep_cross_validation_predictions=TRUE,
base_models = list(best_gbm,
best_drf,
best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(best_gbm,
best_drf,
best_dl))
best_gbm%>%h2o::summary()
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(best_gbm,
best_drf,
best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
base_models = list(best_gbm,
best_drf,
best_dl))
aml <- h2o.automl(x = myX, y = myY,
training_frame = train,
validation_frame = valid,
max_models = 20,
seed = 1)
lb <- h2o.get_leaderboard(aml, extra_columns = "ALL")
lb <- aml@leaderboard
print(lb, n = nrow(lb))
best_auto <- aml@leader
best_auto_perf <- h2o.performance(model = best_auto,
newdata = test)
print(best_auto_perf)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = test,
base_models = list(best_gbm,
best_drf,
best_dl))
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
validation_frame = valid,
base_models = list(best_gbm,
best_drf,
best_dl))
?v
?h2o.automl
?v
?h2o.automl
