---
title: "Data exploration for modeling property price"
author: "LYT"
date: "2019.3.21"
output:
  html_document: default
  pdf_document: default
---
##Update 25 March
Talked to Chief editor of International Journal of Data Analytics: Web of Science, Scopus, Inspec, PsycINFO, Ei Compendex, and so on

##Project objective
####price estimation, updating past property worth records in the dataset to the present value
I will try to build a model that is able to predict current prices of houses in record, based on their attribute, spatial information and historical transactions to reflect the market value of them. This helps to capture the economic risk of tidal flooding in a more intuitive way.

##Timeline
late Mar: a trial machine learning algorithm (Langyi more on this part)

May: a good visualization (also for class project, Bin and Julia will be more involved here)

##Methodology
Can try simple tree, random forest, and boosting) first for now. Random variable selection limitation can help to reduce correlated features issue. Can have variable importance measure. More robust model.

No dimentionality reduction for now, cuz there are not so many variables yet.

Will use package randomForest and caret.

Feed multiple models across parcels. Consider ZIP/county/metrolevel or other indicators of neighbourhood. But whether to use the same group of variables can be an issue since some variables miss by county.

How to deal with variables that change over time? Variable of time can be put into model itself, but introducing others will bring covariates.

If use CV: training vs. testset, how to divide? Across parcels? Look at descriptive statistics across counties to think about heterogenity (there's difference between small/big/rural/urban counties). What about 10-fold or LOOCA?

##litrature review:
In terms of property valuation, mainstream method is parametric hedonic regression. Machine learning came into application recently. 2 papers are the most relevant for now.

Barr et al. (2017) used gradient boosting trees (offers some interpretability) to estimate individual home price at each periods to constuct a house price index. They suggested that local aggregation (metro, county, state, etc.) is more appropriate than global aggregation, as local trends depart from general trend from time to time. They raised the idea of "submarket" as cohort of houses that competing for the same group of people. 
Therefore, they run many millions of models across geographic hierarchies (but didn't say more specifically). They didn't mention the variables they are using and whether they perform data reduction though.

Garcia-Magarino et al. (2019) tested several machine learning and dimensionality reduction methods to address the problem of estimating the missing prices of a sample of houses. They tried OLS, KNN, SVR (an adaptation of SVM), and Artificial neural networks. Dimensionality reduction methods included Non-negative Matrix Factorization, Recursive Feature Elimination, and Forward Selection.

##Data description
There are three sets of data records utilized in the project:

1. home attribute data (codebook: 5.0 Tax Assessor Layout)
2. sales records/transactions (codebook: 5.0 Recorder Layout)
3. flooding risk / environmental variables parcel level variables (parcel risk and spatial data)
and one set of polygon parcel boundaries for most of the parcels in these three datasets (sef_parcels.zip)

The parcel attribute and sales data both have an identifier (attomid) for each property. The parcel risk / spatial data file can be joined to this data as it also contains attomid. The parcel polygons and parcel risk / spatial data both have another id (fsid / firststreetid) that can be used to combine each unique parcel.

The parcel risk  / spatial data file contains fields that represent the inundation risk with field lengths of 6 or 8, e.g. ltc118, rdkt27, mdc118. The first two characters (lt, rd, md, np) represent whether the statistic is about the proportion of the lot, the proportion of roads nearby, the max depth of inundation on the lot (ceiled to feet), or the proportion of nearby properties impacted. The next two characters (kt, em, c1, c3, c5) represent the risk type: kt for repeated king tides, em for highest annual tide, and c1, c3, c5 for hurricane types. The next two characters represent the year for the risk, 18 for 2018, 23 for 2023, etc. If you find te or qu as characters 7 and 8, it identifies the spatial radius used for the measure, tenth of a mile or a quarter mile.

Considering the input variables, tax data can be used to adjust for market price in cases of missing information; the home attributes data vary by county so we should consider hierarchical modelling, if the trial model reveals significance of the unique variables; need to find a proper way to aggregate environment data.

#The home characteristics data
####In local desktop I only imported 10000 obs for trial.

```{r, echo=FALSE, warning=FALSE, echo=FALSE}
library(tidyr)
library(dplyr)
library(data.table)
home_dta<-fread("D:/raw_data/SF_Home_Characteristics.csv")
```

###Brief summary
```{r, echo=FALSE, warning=FALSE}
cat("Data include state:",
    unique(home_dta$situsstatecode)%>%as.character(),
    ",and county:",
    unique(home_dta$situscounty)%>%as.character(),
    "In each state the number of samples are"
    )
```

```{r, echo=FALSE, warning=FALSE}
group_by(home_dta,situscounty)%>%summarise(n(),
                                           mean(assessorlastsaleamount,na.rm = TRUE),
                                           mean(areabuilding,na.rm = TRUE)
                                           )
```
Considerable level of heterogenity by county might exist in data.


###Select and recode useful variables
```{r, echo=FALSE, warning=FALSE}
home_dta<-select(home_dta,
                 attomid,
                 deedlastsaleprice,
                 situsstatecode,
                 situscounty,
                 ownertypedescription1,
                 ownertypedescription2,
                 yearbuilt,
                 propertyusegroup,
                 deedlastsaledate,
                 areabuilding,
                 roomsatticflag,
                 parkinggarage:communityrecroomflag)

#Excluding<$1000 transactions which are not authentic
home_dta$deedlastsaleprice[home_dta$deedlastsaleprice<1000]<-NA
home_dta$deedlastsaleprice<-home_dta$deedlastsaleprice%>%
  as.numeric()
#Owner type recoding misseallenous to NA
home_dta$ownertypedescription1[home_dta$ownertypedescription1=="NP"]<-NA
home_dta$ownertypedescription1[home_dta$ownertypedescription1=="UNKNOWN"]<-NA
home_dta$ownertypedescription2[home_dta$ownertypedescription1=="NP"]<-NA
home_dta$ownertypedescription2[home_dta$ownertypedescription1=="UNKNOWN"]<-NA
#Recoding property use group
home_dta$propertyusegroup[home_dta$propertyusegroup=="UNKNOWN"|
                            home_dta$propertyusegroup=="Other"|
                            home_dta$propertyusegroup=="NP"]<-NA
#152 PropertyUseStandardized is better coded by
class_coding<-read.csv("D:/raw_data/prop_use_codes_trim.csv")
#Rounding deed last sale date to year and recoding NAs
library(stringr)
home_dta$assessorlastsaledate<-str_sub(home_dta$assessorlastsaledate,start = 0,end = 4)%>%
  as.numeric()
home_dta$assessorlastsaledate[home_dta$assessorlastsaledate==""]<-NA
#Excluding <50 sq. feet living area
home_dta$areabuilding[home_dta$areabuilding<50]<-NA
#Recoding parkinggarage (?)
home_dta$parkinggarage[home_dta$parkinggarage=="11"|
                                                 home_dta$parkinggarage=="12"|
                                                 home_dta$parkinggarage=="18"|
                                                 home_dta$parkinggarage=="40"|
                                                 home_dta$parkinggarage=="999"]<-NA
#Other variables yet to recode
```

###Modeling by county
```{r, echo=FALSE, warning=FALSE}
home_county_dta<-split(home_dta,home_dta$situscounty)
```

```{r, echo=FALSE, warning=FALSE}
#Function to determine whether a variable is missing over 10% values
is.missing<-function(x){
    a<-x%>%length()
    b<-x%>%is.na()%>%sum()
    if (b/a<0.1){
      return(TRUE)
    }
    else{
      return(FALSE)
    }
  }

#Function to preprocess a data frame and drop according to is.missing
drop.missing<-function(x){
  for (i in names(x)){
    if (is.missing(x[[i]])==FALSE){
      x<-select(x,-i)
    }
  }
  x<-x
}
```
```{r, echo=FALSE, warning=FALSE}
home_county_dta$Broward<-drop.missing(home_county_dta$Broward)
home_county_dta$Collier<-drop.missing(home_county_dta$Collier)
```

```{r, echo=FALSE, warning=FALSE}
#Apply to all counties
lapply(home_county_dta,drop.missing)
```




#Transaction data

trx_dta<-fread("D:/raw_data/SF_Sales_Transactions_Data.csv")


The variable useful here is pretty straightforward: transaciton price 23 TransferAmount. After recoding it with "kick out <$1000" methods (this already kicks out nearly half in the sample) the result is still not so satisfying. Maybe think about adjust according to price per sq feet?


trx_dta$transferamount[trx_dta$transferamount<1000]<-NA
trx_dta$transferamount%>%sort(decreasing=FALSE)%>%unique()%>%head(10)
trx_dta$transferamount%>%summary()


Regarding sales time, 2 variables look like compensating each other, but actually not sure: 13 InstrumentDate
and 14 RecordingDate.

##Parcel/risk data

risk_dta<-fread("D:/raw_data/SF_Parcel_Risk_and_Spatial_Data.csv")


One thing I'm still not clear is the geographical mapping that has been made. I understand one parcel division in the paper--properties with different levels of flooding risk. But you also mention Inverse Distance Weighted (IDW) to build value surfaces--is this automatic or manually tuned? Can we use it to construct neibourhood parcels that divides between models?

Besides, from 132 Totpopbg to 145 Hisptr seems to record ethnicity background information. How is that recorded and what's the unit?

###A series of interesting variables

for (i in 132:180){
  a<-risk_dta[,i]%>%length()
  b<-risk_dta[,i]%>%is.na()%>%sum()
  if (b/a>0.1){
    print(colnames(risk_dta[i]))
  }
}

Apart from MedIncbg are all missing, other variables are pretty complete for analysis.

