---
title: "Data exploration for modeling property price"
author: "Langyi Tian"
date: "2019.4.5"
output:
  html_document: default
  pdf_document: default
---
#### Update 5 April
#### To do list for next week
1. Could try model 5th order polynomial on (sale year - 2005) then feed estimates back to the model to control for time effect  *risk variables not relevant with past time
2. try ridge/LASSO with all interaction and feature selection (especially census tract and time, also can include ZIP code as variable)
3. recode NA and keep the positive response to increase completeness
4. What about tune the filtering parameter for each model?

## Introduction
The price valuation of real estate property has been for long the interest of investment researchers. Mainstream method has been parametric hedonic regression. Machine learning came into application recently. 2 papers are the most relevant for now.

Barr et al. (2017) used gradient boosting trees (offers some interpretability) to estimate individual home price at each periods to constuct a house price index. They suggested that local aggregation (metro, county, state, etc.) is more appropriate than global aggregation, as local trends depart from general trend from time to time. They raised the idea of "submarket" as cohort of houses that competing for the same group of people. Therefore, they run many millions of models across geographic hierarchies (but didn't say more specifically). They didn't mention the variables they are using and whether they perform data reduction though.

Garcia-Magarino et al. (2019) tested several machine learning and dimensionality reduction methods to address the problem of estimating the missing prices of a sample of houses. They tried OLS, KNN, SVR (an adaptation of SVM), and Artificial neural networks. Dimensionality reduction methods included Non-negative Matrix Factorization, Recursive Feature Elimination, and Forward Selection.

This project builds on these existing research. It builds from scratch machine learning algorithms that estimates property value in a large-scale administrative dataset containing 3 million houses in Florida. More imprtantly, it aims to add to the First Street Foundation's data science team's algorithm that presents to the target audience the economic risk of tidal flooding. When accurate prediction of property value fluctuation can be realized, a much more realistic presentation of the risk can be made subsequently.

## Data description
*The data would require at least 8G RAM to be stored which means it cannot be runned on most local devices. For this project the codes are runned in the cloud environment of a Amazon Web Service EC2 instance.
There are two sets of data records utilized in the project. The range is limited to counties in South Florida.

1. Administrative data of all properties (codebook: 5.0 Tax Assessor Layout) and sales records/transactions (codebook: 5.0 Recorder Layout) that is purchased from ATTOM Data Solutions.
2. flooding risk / environmental variables parcel level variables (parcel risk and spatial data)
and one set of polygon parcel boundaries for most of the parcels in these three datasets (sef_parcels.zip), that is constructed by First Street Foundation data science team.

#### Input original data
```{r, setup, echo=TRUE, warning=FALSE}
library(tidyr)
library(dplyr)
library(stringr)
library(purrr)
library(ggplot2)
library(data.table)
library(gridExtra)
```

```{r, eval=FALSE, warning=FALSE}
#Input original data
home_dta_original<-fread("D:/raw_data/SF_Home_Characteristics.csv") #This is original ATTOM data
#Select features to import
home_dta<-select(home_dta_original,
                 attomid,#Matching ID
                 deedlastsaleprice,#Transaction price last sale
                 situsstatecode,#State code
                 situscounty,#County code
                 propertyaddresscity,#City code
                 ownertypedescription1,#First owner is individual/company?
                 ownertypedescription2,#Second owner is individual/company?
                 deedlastsaledate,#Date of market sale
                 yearbuilt,#Year when built
                 propertyusegroup,#Commercial/residential?
                 areabuilding,#Living area in sq.feet
                 censustract,#Census tract division
                 propertylatitude,#Lat
                 propertylongitude,#Lon
                 roomsatticflag,#See below
                 parkinggarage:communityrecroomflag#A series variable measuring physical attributes of the property, including rooms count and relevant facilities
                 )
save(home_dta,file = "D:/Langyi/Taxassessor_reduced.RData")
```

### Select and recode variables at general level
Coming from the tax authority, the data contains a lot of useful information but at the same time lots of noise. Most measurements created by tax bureau, mainly legal registration and taxation information, are not utilized in researching the problem that this project looks at. Those variables are therefore dropped.
However, a series of variable proved to be useful.
```{r, echo=TRUE, warning=FALSE}
home_data<-load("D:/Langyi/Taxassessor_reduced.RData")
#Store a stage version of data here
report.dim<-function(dta){
  cat("Up to here, the data dimension is",
    dim(dta)[1],
    "*",
    dim(dta)[2]
    )
}
home_dta_ver1<-home_dta
home_dta_ver1%>%report.dim()
```

```{r, echo=TRUE, warning=FALSE}
#This chunk of recoding is not supposed to change data dimension
#Fill in the price variable so that it will not be dropped later
home_dta$deedlastsaleprice[is.na(home_dta$deedlastsaleprice)=="TRUE"]<-0
#Rounding deed last sale date to year and recoding NAs
home_dta$deedlastsaledate<-str_sub(home_dta$deedlastsaledate,start = 0,end = 4)%>%
  as.numeric()
home_dta$deedlastsaledate[home_dta$deedlastsaledate==""]<-NA
#Make id numeric
home_dta$attomid<-home_dta$attomid%>%as.numeric()
#Recoding ownership type misseallenous to NA
home_dta$ownertypedescription1[home_dta$ownertypedescription1=="NP"]<-NA
home_dta$ownertypedescription1[home_dta$ownertypedescription1=="UNKNOWN"]<-NA
home_dta$ownertypedescription2[home_dta$ownertypedescription1=="NP"]<-NA
home_dta$ownertypedescription2[home_dta$ownertypedescription1=="UNKNOWN"]<-NA
#Recoding property usage group
home_dta$propertyusegroup[home_dta$propertyusegroup=="UNKNOWN"|
                            home_dta$propertyusegroup=="Other"|
                            home_dta$propertyusegroup=="NP"]<-NA
#Note that 152 PropertyUseStandardized is better coded by, should do this in future
class_coding<-read.csv("D:/raw_data/prop_use_codes_trim.csv")
#Excluding <50 sq. feet living area
home_dta$areabuilding[home_dta$areabuilding<50]<-NA
#As tree modeling does not accept character vector, those in the data are recoded to factors.
cat("These variables are recoded to factors")
for (i in names(home_dta)){
  if (class(home_dta[[i]])=="character"){
    print(i)
    home_dta[[i]]<-home_dta[[i]]%>%as.factor()
  }
}
```

Considering improving the accuracy of prediction, one difficult choice I have to make for now is to only keep houses that have been transacted after 2012 in our dataset for modeling. There're several reasons to justify this choice. First, the housing market fluctuates overtime, and it's very hard to capture this time-series effect in the modeling, at least at initial stage. Second, the independent features across time change in its nature and suffer from covariation issue. The physical attributes such as room size might have compeletely different contexts in 2010 compared with 1960. Therefore, according to Florida's house market history, I simply pick up a relatively period where properties are generally moderately appreciating, and it turns out to be the period since 2012 until this point (2019). This selection had unavoidably omit much useful information, but it reduces noise a lot at the same time, and the sample size has not been substantially reduced. Therefore, at this stage I think it's generally acceptable.

During recoding, I cleared 5 variables that registers some details such as roof and floor material as they come as factor variables with many levels which cannot be easily reduced. To prevent their impact on the tree algorithm, they are kept from modeling for this moment. Below prints their name.

```{r, echo=FALSE, warning=FALSE}
#Drop variables with too many levels, besides those are numerical
cat("These variables are excluded due to too many levels")
for (i in names(home_dta)[-1:-14]){
    if ((home_dta[[i]]%>%unique()%>%length()>3)&
        (str_detect(i,pattern = "area|count")==FALSE)
        ){
    print(i)
    home_dta<-home_dta%>%select(-i)
    }
}
#Store a stage version of data here
home_dta_ver2<-home_dta
home_dta_ver2%>%report.dim()
```
### Some exploratory analysis
```{r, echo=TRUE, warning=FALSE}
#Plot summary of a few important variables
summary.numeric<-function(varname="deedlastsaleprice",
                        dta=home_dta){
m<-ggplot(data=dta)+
  geom_area(aes(x = dta[[varname]]),
                 stat = "bin",
            fill = "khaki2"
           )+
  xlab(varname)+
  ylab("Properties")+
  theme_classic()
return(m)
}

summary.categorical<-function(varname="propertyusegroup",
                        dta=home_dta){
m<-ggplot(data=dta)+
  geom_bar(aes(x = dta[[varname]]),
            fill = "khaki2"
           )+
  xlab(varname)+
  ylab("Properties")+
  theme_classic()
return(m)
}

grid.arrange(summary.numeric("deedlastsaledate"),
             summary.numeric("deedlastsaleprice"),
             summary.numeric("yearbuilt"),
             summary.numeric("areabuilding"),
             summary.categorical("propertyusegroup"),
             summary.categorical("ownertypedescription1"),
             summary.categorical("ownertypedescription2"),
             ncol=2)
```

```{r,warning=FALSE,message=FALSE,echo=TRUE}
#Import shape file and save to rdata to reduce loading time
library(rgdal)
parcel<-readOGR(dsn = "D:/Langyi/tl_2017_us_county", layer = "tl_2017_us_county")
counties<-c("Broward","Collier","Hendry","Lee","Miami-Dade","Monroe","Palm Beach")
parcel<-parcel[parcel@data$NAME%in%counties&parcel@data$STATEFP=="12",]
```

```{r,warning=FALSE,message=FALSE,echo=TRUE}
#Summary a few key variables by county
county_summary<-home_dta%>%group_by(situscounty)%>%summarize(n()%>%as.numeric(),
                                                    mean(deedlastsaleprice%>%as.numeric(),na.rm = TRUE),
                                             mean(yearbuilt%>%as.numeric(),na.rm = TRUE),
                                          mean(areabuilding%>%as.numeric(),na.rm = TRUE))
names(county_summary)<-c("NAME","Num_of_Properties","Average_Price","Average_Built_Year","Average_Living_Area")
parcel@data<-inner_join(parcel@data,county_summary,by="NAME")
```

```{r,warning=FALSE,message=FALSE,echo=TRUE}
#Plot statistics by county
library(tmap)
summary.county<-function(varname,par=parcel){
m<-tm_shape(parcel) + 
  tm_fill(varname)+
  tm_text("NAME",size = 0.6)+
  tm_borders()
return(m)
}
tmap_arrange(summary.county(varname="Num_of_Properties"),
             summary.county(varname="Average_Living_Area"),
             summary.county(varname="Average_Price"),
             summary.county(varname="Average_Built_Year"),
             asp = 1)
```

### Seperate modeling for layered administrative units

Another important choice is we construct a seperate model for a parcel in every county, instead of include all or most of them in a general model. The distinction  could be county at largest, city-level, ZIP code level, or census tract level (better as factor).

This is partly inspired by the "submarket" idea, as the housing prices between neibourhoods, cities and counties vary a lot. Also, a unique characteristic of the dataset also contributes to this solution--There are some variables that represent housing attributes that miss in data with different cases between counties. In this case, as the input variables vary, counties has to be seperated when modeling. 

####3.3.1 Subgroup by cities

The below table prints 15 cities with the most properties, and the area plot shows the rest. We can see there are many small towns with very few obs.
```{r, echo=TRUE, warning=FALSE}
#The city variable is messy, so need to visualize first
city_obs<-group_by(home_dta,propertyaddresscity)%>%summarise(n())
names(city_obs)[1]<-"City"
names(city_obs)[2]<-"Properties"
city_obs[order(city_obs$Properties,decreasing = TRUE),]%>%head(15)%>%data.table()
city_obs<-city_obs%>%filter(city_obs$Properties<20000)
ggplot(data=city_obs)+
  geom_area(aes(x = city_obs$Properties),
                 stat = "bin",
            fill = "khaki2"
           )+
  xlab("Numbers of properties in data")+
  ylab("Number of cities")+ 
  ggtitle("Most cities have few total number of properties")+
  theme_classic()
#For obtaining sufficient number of obs in each model, we drop cities with <1000 obs.
city_obs<-city_obs[-1,]
city_obs<-city_obs%>%filter(city_obs$Properties>1000)%>%as.data.frame()
city_obs$Properties<-NULL
names(city_obs)[1]<-"propertyaddresscity"
home_dta_less_city<-home_dta%>%inner_join(city_obs,key = "prepertyaddresscity")
#Store a stage version of data here
home_dta_ver3<-home_dta
home_dta_ver3%>%report.dim()
#Split data
home_dta_less_city$propertyaddresscity<-home_dta_less_city$propertyaddresscity%>%as.character()
home_city_dta<-split(home_dta_less_city,home_dta_less_city$propertyaddresscity)
```

#### Subgroup by counties
Counties are rather clean in strucutre, and every county has a decent number of samples.
```{r, echo=TRUE, warning=FALSE}
home_county_dta<-split(select(home_dta,-censustract),home_dta$situscounty)
```

### Feature selection with individual models
Due to the issue of different missing data patter in different counties, I then further select variable based on each subgroup that will be used to constructed the model. This for now includes drop variables with too many missing values and drop variables whose value is not varying (such as all 0 for numerical features, or all at one level for factor variables).

```{r, echo=TRUE, warning=FALSE}
#Function to determine whether a variable is missing less than 10% values
is.missing<-function(var,ratio = 0.1){
    a<-var%>%length()
    b<-var%>%is.na()%>%sum()
    b/a < ratio
}
#Function to preprocess a data frame and drop according to is.missing
#Here some features are kept due to their apparent relevancy which does not worth being dropped due to missing values
drop.missing<-function(dta){
  varnames_important<-names(dta)[1:14]
  varnames_supplement<-names(dta)[-1:-14]
  for (i in varnames_important){
    var<-dta[[i]]
    if (is.missing(var,ratio = 0.5)=="FALSE"){
      dta<-select(dta,-i)
    }
  }
  for (i in varnames_supplement){
    var<-dta[[i]]
    if (is.missing(var,ratio = 0.1)=="FALSE"){
      dta<-select(dta,-i)
    }
  }
  return(dta)
}
```

```{r, echo=TRUE, warning=FALSE}
#Drop unary variables
drop.unary<-function(dta){
  for (i in names(dta)[-1:-5]){
    var<-dta[[i]]
    var<-var[is.na(var)==FALSE]
    if ((var%>%unique()%>%length()==1)|
        (var%>%unique()%>%length()==0)){
    dta<-dta%>%select(-i)
    }
  }
  return(dta)
}
```

```{r, echo=TRUE, warning=FALSE}
#Apply to city the function in previous function for feature selection
home_city_cleaned_dta<-purrr::map(home_city_dta,drop.missing)
home_city_cleaned_dta<-purrr::map(home_city_cleaned_dta,drop.unary)
#Apply to county the function in previous function for feature selection
home_county_cleaned_dta<-purrr::map(home_county_dta,drop.missing)
home_county_cleaned_dta<-purrr::map(home_county_cleaned_dta,drop.unary)
```

```{r, echo=TRUE, warning=FALSE}
#Provide an interactive leaflet for checking individual properties, Aventura as an example.
map.variable<-function(dta=home_city_cleaned_dta$`AVE MARIA`,
                       varname="propertyusegroup"){
if (varname%in%names(dta)==TRUE&
    "propertylatitude"%in%names(dta)==TRUE&
    "propertylongitude"%in%names(dta)==TRUE){
  dta<-dta[is.na(dta[[varname]])==FALSE,]
library(leaflet)
pal = colorFactor("Set1", dta[[varname]])
color_offsel1 = pal(dta[[varname]])
leaflet(dta) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~propertylongitude, 
             lat = ~propertylatitude,
             color = color_offsel1,
             popup = content<<-str_c(varname,
                                     dta[[varname]],
                                     "Market price",
                                     dta$deedlastsaleprice,
                                     "Transaction date",
                                     dta$deedlastsaledate,
                                     "Living area",
                                     dta$areabuilding,
                                     sep = " "),
             clusterOptions = markerClusterOptions())%>%
  addLegend(pal = pal, values = ~dta[[varname]], title = varname)
} else {print("Variables filtered out in final data, unable to plot map for this subsample")
}
}
map.variable(dta = home_city_cleaned_dta$AVENTURA)
```

### Build predictive models for individual subgroups

At first stage, we chose to use a series of tree algorithms to try the modeling, namely simple tree, random forest and gradient boosting. The reason is the high interpretability of tree algorithms. For now, the focus is supposed to be identify variables that are importance, or that requires further improvement or attention. To this extent, the way that tree algorithms returns relative importance would help us with this. Moreover, random forest is especially appropriate in the case of our data, as many attributes are correlated (such as living area and room counts), which might introduce inaccuracy in projection. By iterating with partial set of variables, random forest can provide relatively robust insights with respect to which variables deserve more attention. 

#### Final preprocessing before feeding data into model
```{r, echo=TRUE, warning=FALSE}
#Build final preprocess function
model.preprocess<-function(dta,
                           price_high,
                           price_low,
                           start_year,
                           size){
#Delete lng/lat
dta$propertylatitude<-NULL
dta$propertylongitude<-NULL
#Recode tract to factor
  if ("censustract"%in%names(dta)){
    if (dta$censustract%>%unique()%>%length()<32){
      dta$censustract<-dta$censustract%>%as.factor()
    }
    else {
      dta<-dta%>%select(-censustract)
    }
  }
#Exclude NA in y variable
dta<-filter(dta,is.na(dta$deedlastsaleprice)==FALSE)
row_before<-nrow(dta)
#Excluding high value transactions to suppress extreme values for convenience
dta$deedlastsaleprice[dta$deedlastsaleprice>=price_high]<-NA
#Excluding low value transactions which are not authentic
dta$deedlastsaleprice[dta$deedlastsaleprice<=price_low]<-NA
dta$deedlastsaleprice<-dta$deedlastsaleprice%>%
  as.numeric()
#Exclude NA in y variable again
dta<-filter(dta,is.na(dta$deedlastsaleprice)==FALSE)
row_after<-nrow(dta)
retention<<-row_after/row_before
#As past transaction reflect less information, we only pick recent years (after 2012)
if ("deedlastsaledate"%in%names(dta))
dta<-dta%>%filter(deedlastsaledate>=start_year)
#Reduce size for computation convenience
set.seed(0)
  if (nrow(dta)>=size){
    dta<-sample_n(dta,size)
  }
return(dta)
}
```

#### Compare results from simple tree, random forest and gradient boosting for each subgroup
```{r, echo=TRUE, warning=FALSE}
model.comparison<-function(dta,preprocess_retention=retention){
#Split training/test samples (0.7:0.3)
train<-sample_frac(dta,size=0.7)
test<-anti_join(dta,train,by="attomid")
#Prepare y and x features
train<-select(train,
          -attomid,
          -situsstatecode,
          -situscounty,
          -propertyaddresscity)
y<-train$deedlastsaleprice
x<-select(train,
          -deedlastsaleprice)
#Try random forest
library(randomForest)
train_rf <- randomForest(y~.,x,
                         importance = TRUE,
                         na.action = "na.omit"
                         )
rf_summary<-importance(train_rf)%>%as.data.frame()
rf_highvars<-rf_summary[order(rf_summary$`%IncMSE`, 
                 decreasing = TRUE),]%>%head(5)%>%row.names()
if (length(rf_highvars)<5){
  add<-5-length(rf_highvars)
  rf_highvars<-c(rf_highvars,rep(NA,add))
}
#Test set
yhat.rf <- predict(train_rf,test)
rf_MSE<-mean((yhat.rf-test$deedlastsaleprice)^2,na.rm = TRUE)
rf_MPE<-mean((yhat.rf-test$deedlastsaleprice)/test$deedlastsaleprice,na.rm = TRUE)
rf_MAPE<-mean(abs((yhat.rf-test$deedlastsaleprice)/test$deedlastsaleprice),na.rm = TRUE)
#Boosting
library(gbm)
train_gb <- gbm(y~.,x,
                n.trees = 1000,
                    distribution = "gaussian"
)
gb_summary<-summary(train_gb,plotit = FALSE)
gb_highvars<-gb_summary[order(gb_summary$rel.inf, 
                 decreasing = TRUE),]%>%head(5)%>%row.names()
if (length(gb_highvars)<5){
  add<-5-length(gb_highvars)
  gb_highvars<-c(gb_highvars,rep(NA,add))
}
#Test set
yhat.gb <- predict(train_gb,test,n.trees = 1000)
gb_MSE<-mean((yhat.gb-test$deedlastsaleprice)^2,na.rm = TRUE)
gb_MPE<-mean((yhat.gb-test$deedlastsaleprice)/test$deedlastsaleprice,na.rm = TRUE)
gb_MAPE<-mean(abs((yhat.gb-test$deedlastsaleprice)/test$deedlastsaleprice),na.rm = TRUE)
#Store comparison between rf and gb
if (dta$propertyaddresscity%>%unique()%>%length()==1){
  subsample_name<-paste(dta$situsstatecode%>%unique()%>%as.character()%>%paste(collapse ="_"),
                      dta$situscounty%>%unique()%>%as.character()%>%paste(collapse ="_"),
                      dta$propertyaddresscity%>%unique()%>%as.character()%>%paste(collapse ="_"),
                      sep = "_")
} else {
  subsample_name<-paste(dta$situsstatecode%>%unique()%>%as.character()%>%paste(collapse ="_"),
                        dta$situscounty%>%unique()%>%as.character()%>%paste(collapse ="_"))
  
}
comparison<-c(subsample_name,
              preprocess_retention,
              nrow(dta),
              ncol(dta),
              rf_MSE,
              rf_MPE,
              rf_MAPE,
              rf_highvars,
              gb_MSE,
              gb_MPE,
              gb_MAPE,
              gb_highvars)%>%as.data.frame%>%t()
colnames(comparison)<-c("subsample",
                        "preprocess_retention",
                        "obs",
                        "vars",
                        "rf_MSE",
                        "rf_MPE",
                        "rf_MAPE",
                         "rf_1st",
                         "rf_2nd",
                         "rf_3rd",
                         "rf_4th",
                         "rf_5th",
                        "gb_MSE",
                        "gb_MPE",
                        "gb_MAPE",
                         "gb_1st",
                         "gb_2nd",
                         "gb_3rd",
                         "gb_4th",
                         "gb_5th")
return(comparison)
}
```

#### Assemble final modeling function
```{r, echo=TRUE, warning=FALSE}
#Build function to parse modeling output and realize parameter entry
modeling<-function(list,
                   price_h=5000000,
                           price_l=100,
                           yr_st=2012,
                           sz=500){
comparison_set<-NULL
for (i in 1:length(list)){
  dta<-list[[i]]
  dta_preprocessed<-model.preprocess(dta,
                                     price_high = price_h,
                                     price_low = price_l,
                                     start_year = yr_st,
                                     size = sz)
  comparison<-model.comparison(dta_preprocessed)
  comparison_set<-rbind(comparison_set,comparison)
}
  comparison_set<-cbind(
    rep(price_h,nrow(comparison_set)),
    rep(price_l,nrow(comparison_set)),
    rep(yr_st,nrow(comparison_set)),
    rep(sz,nrow(comparison_set)),
    comparison_set
  )
  comparison_set<-comparison_set%>%as.data.frame()
  names(comparison_set)[1:4]<-c("price_high","price_low","year_start","size")
  comparison_set$preprocess_retention<-comparison_set$preprocess_retention%>%as.character()%>%as.numeric()
  for (i in c("obs",
              "vars",
              "rf_MSE",
              "rf_MPE",
              "rf_MAPE",
              "gb_MSE",
              "gb_MPE",
              "gb_MAPE")){
    comparison_set[[i]]<-comparison_set[[i]]%>%as.character()%>%as.numeric()
              }
  return(comparison_set)
}
```
###3.6 Test between random forest and gradient boosting
```{r, echo=TRUE, warning=FALSE}
model_city<-modeling(home_city_cleaned_dta)
model_county<-modeling(home_county_cleaned_dta)
```

```{r, echo=TRUE, warning=FALSE}
cat("In",
    nrow(model_city),
    "city-level subsamples,",
    model_city[model_city$rf_MSE<model_city$gb_MSE,]%>%nrow(),
    "generate better performance measure by MSE with random forest than boosting trees.",
    model_city[model_city$rf_MPE<model_city$gb_MPE,]%>%nrow(),
    "by MPE.",
    model_city[model_city$rf_MAPE<model_city$gb_MAPE,]%>%nrow(),
    "by MAPE."
    )

cat("In",
    nrow(model_county),
    "county-level subsamples,",
    model_county[model_county$rf_MSE<model_county$gb_MSE,]%>%nrow(),
    "generate better performance measure by MSE with random forest than boosting trees.",
    model_county[model_county$rf_MPE<model_county$gb_MPE,]%>%nrow(),
    "by MPE.",
    model_county[model_county$rf_MAPE<model_county$gb_MAPE,]%>%nrow(),
    "by MAPE."
    )
```

### Personalize filtering parameters
For next step, fixing the machine learning model to be random forest, we want to see how we filter observations make an impact on prediction accuracy. This include mainly how we filter y variable-the price, sample size, and how many older properties to filter out.
I achieve this by looping over a range of a given parameter and look at average MSE of individual county models. Here I use county models as city models are much more idiosyncratic in features and county models are sort of more robust.
```{r, echo=TRUE, warning=FALSE}
#Build function to plot series of error for a given tuning parameter
error.plot<-function(dta=error_matrix){
retention<-ggplot(data=dta)+
  geom_line(aes(x = Parameter,
                y = Retention),
            color = "tomato1"
           )+
  xlab("Parameter values")+
  theme(legend.position = "left")+
  theme_classic()
mse<-ggplot(data=dta)+
  geom_line(aes(x = Parameter,
                y = MSE),
            color = "tomato2"
           )+
  xlab("Parameter values")+
  theme(legend.position = "left")+
  theme_classic()
mpe<-ggplot(data=dta)+
  geom_line(aes(x = Parameter,
                y = MPE),
            color = "tomato3"
           )+
  xlab("Parameter values")+
  theme(legend.position = "left")+
  theme_classic()
mape<-ggplot(data=dta)+
  geom_line(aes(x = Parameter,
                y = MAPE),
            color = "tomato4"
           )+
  xlab("Parameter values")+
  theme(legend.position = "left")+
  theme_classic()
grid.arrange(retention,mse,mape,mpe)
}
```
####  Sample size limit in individual models

```{r, eval=TRUE, echo=FALSE, warning=FALSE}
error_matrix<-NULL
for (i in c(100,300,500)){
output<-modeling(home_county_cleaned_dta,
                      price_h=5000000,
                      price_l=100,
                      sz=i,
                      yr_st=2012)
error_performance<-cbind(i,
                         output$preprocess_retention%>%mean(),
                         output$rf_MSE%>%mean(),
                         output$rf_MPE%>%mean(),
                         output$rf_MAPE%>%mean()
)
error_matrix<-rbind(error_matrix,error_performance)
}
colnames(error_matrix)<-c("Parameter","Retention","MSE","MPE","MAPE")
error_matrix<-error_matrix%>%as.data.frame()
error.plot(error_matrix)
```
#### Highest price
```{r, eval=FALSE, echo=FALSE, warning=FALSE}
error_matrix<-NULL
for (i in c(50000,100000,500000,1000000)){
output<-modeling(home_county_cleaned_dta,
                      price_h=i,
                      price_l=100,
                      sz=200,
                      yr_st=2012)
error_performance<-cbind(i,
                         output$preprocess_retention%>%mean(),
                         output$rf_MSE%>%mean(),
                         output$rf_MPE%>%mean(),
                         output$rf_MAPE%>%mean()
)
error_matrix<-rbind(error_matrix,error_performance)
}
colnames(error_matrix)<-c("Parameter","Retention","MSE","MPE","MAPE")
error_matrix<-error_matrix%>%as.data.frame()
error.plot(error_matrix)
```
#### Lowest price acceptable for property

```{r, echo=TRUE, warning=FALSE}
error_matrix<-NULL
for (i in c(100,300,500,1000,7000,10000)){
output<-modeling(home_county_cleaned_dta,
                      price_h=5000000,
                      price_l=i,
                      sz=200, 
                      yr_st=2012)
error_performance<-cbind(i,
                         output$preprocess_retention%>%mean(),
                         output$rf_MSE%>%mean(),
                         output$rf_MPE%>%mean(),
                         output$rf_MAPE%>%mean()
)
error_matrix<-rbind(error_matrix,error_performance)
}
colnames(error_matrix)<-c("Parameter","Retention","MSE","MPE","MAPE")
error_matrix<-error_matrix%>%as.data.frame()
error.plot(error_matrix)
```

#### Year to start
```{r, echo=TRUE, warning=FALSE}
error_matrix<-NULL
for (i in c(2000,2007,2010,2013,2015,2016)){
output<-modeling(home_county_cleaned_dta,
                      price_h=5000000,
                      price_l=100,
                      sz=200, 
                      yr_st=i)
error_performance<-cbind(i,
                         output$preprocess_retention%>%mean(),
                         output$rf_MSE%>%mean(),
                         output$rf_MPE%>%mean(),
                         output$rf_MAPE%>%mean()
)
error_matrix<-rbind(error_matrix,error_performance)
}
colnames(error_matrix)<-c("Parameter","Retention","MSE","MPE","MAPE")
error_matrix<-error_matrix%>%as.data.frame()
error.plot(error_matrix)
```

### Iteration through cities/counties and view corss-validated model performance
#### Cities
```{r, echo=TRUE, warning=FALSE}
library(knitr)
rf_set_city<-modeling(home_city_cleaned_dta)%>%select(price_high:rf_5th)
rf_set_city[order(rf_set_city$rf_MAPE),]%>%data.table()
```

#### Counties
```{r, echo=TRUE, warning=FALSE}
rf_set_county<-modeling(home_county_cleaned_dta)%>%select(price_high:rf_5th)
rf_set_county[order(rf_set_county$rf_MAPE),]%>%data.table()
```

### Map county-level model performance
```{r, echo=TRUE, warning=FALSE}
#merge model performance data by county to parcel file
rf_set_county1<-rf_set_county
rf_set_county1$subsample<-rf_set_county1$subsample%>%str_sub(start = 4L)
names(rf_set_county1)[5]<-"NAME"
parcel@data<-inner_join(parcel@data,rf_set_county1,by="NAME")
```

```{r, echo=TRUE, warning=FALSE}
tmap_arrange(summary.county(varname="preprocess_retention"),
             summary.county(varname="obs"),
             summary.county(varname="vars"),
             summary.county(varname="rf_MSE"),
             summary.county(varname="rf_MPE"),
             summary.county(varname="rf_MAPE"),
             summary.county(varname="rf_1st"),
             summary.county(varname="rf_2nd"),
             summary.county(varname="rf_3rd"),
             ncol=3)
```
####Transaction data

trx_dta<-fread("D:/raw_data/SF_Sales_Transactions_Data.csv")

trx_dta%>%group_by(attomid)%>%summarise(n())


The variable useful here is pretty straightforward: transaciton price 23 TransferAmount. After recoding it with "kick out <$1000" methods (this already kicks out nearly half in the sample) the result is still not so satisfying. Maybe think about adjust according to price per sq feet?


trx_dta$transferamount[trx_dta$transferamount<1000]<-NA
trx_dta$transferamount%>%sort(decreasing=FALSE)%>%unique()%>%head(10)
trx_dta$transferamount%>%summary()


Regarding sales time, 2 variables look like compensating each other, but actually not sure: 13 InstrumentDate
and 14 RecordingDate.

####Parcel/risk data
The parcel attribute and sales data both have an identifier (attomid) for each property. The parcel risk / spatial data file can be joined to this data as it also contains attomid. The parcel polygons and parcel risk / spatial data both have another id (fsid / firststreetid) that can be used to combine each unique parcel.

The parcel risk  / spatial data file contains fields that represent the inundation risk with field lengths of 6 or 8, e.g. ltc118, rdkt27, mdc118. The first two characters (lt, rd, md, np) represent whether the statistic is about the proportion of the lot, the proportion of roads nearby, the max depth of inundation on the lot (ceiled to feet), or the proportion of nearby properties impacted. The next two characters (kt, em, c1, c3, c5) represent the risk type: kt for repeated king tides, em for highest annual tide, and c1, c3, c5 for hurricane types. The next two characters represent the year for the risk, 18 for 2018, 23 for 2023, etc. If you find te or qu as characters 7 and 8, it identifies the spatial radius used for the measure, tenth of a mile or a quarter mile.

risk_dta<-fread("D:/raw_data/SF_Parcel_Risk_and_Spatial_Data.csv")


One thing I'm still not clear is the geographical mapping that has been made. I understand one parcel division in the paper--properties with different levels of flooding risk. But you also mention Inverse Distance Weighted (IDW) to build value surfaces--is this automatic or manually tuned? Can we use it to construct neibourhood parcels that divides between models?

Besides, from 132 Totpopbg to 145 Hisptr seems to record ethnicity background information. How is that recorded and what's the unit?

#####A series of interesting variables

for (i in 132:180){
  a<-risk_dta[,i]%>%length()
  b<-risk_dta[,i]%>%is.na()%>%sum()
  if (b/a>0.1){
    print(colnames(risk_dta[i]))
  }
}

Apart from MedIncbg are all missing, other variables are pretty complete for analysis.