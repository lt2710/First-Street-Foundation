---
title: "Estimating Costal Property Values in Florida"
author: "Langyi Tian"
output: 
  xaringan::moon_reader:
    css: ["default", "rutgers-fonts", "duke-blue"]
    lib_dir: libs
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
exclude: true

```{r packages, include=FALSE}
# Load packages.
packages <- c("tidyr", 
              "dplyr",
              "stringr",
              "purrr", 
              "data.table", 
              "maps",
              "glmnet",
              "tree",
              "knitr",
              "ggplot2",
              "gridExtra",
              "rgdal",
              "tmap",
              "leaflet",
              "DT"
              )
packages <- lapply(packages, FUN = function(x) {
  if(!require(x, character.only = TRUE)) {
    install.packages(x)
  library(x, character.only = TRUE)
  }
}
)
```

```{r setup}
library("knitr")
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 15,
  fig.height = 7,
  figure.align = "center",
  cache = TRUE,
  cache.lazy = FALSE
)
#knitr::opts_knit$set(root.dir = "C:\\Users\\Tianl\\Documents\\GitHub\\First-Street-Foundation")
options(htmltools.dir.version = FALSE)
```  

```{r theme-map, include=FALSE}
theme_simplemap <- function(base_size = 9,
                            base_family = "") {
  theme_bw(base_size = base_size, base_family = base_family) %+replace%
    theme(
      axis.line = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      axis.title = element_blank(),
      panel.background = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      panel.spacing = unit(0, "lines"),
      plot.background = element_blank(),
      legend.position = "none"
    )
}
``` 

---
## Executive summary
Objective
 - Organizational mission: Quantify the financial risk of tidal flooding to address stakeholder concern
 - Task for this research: Make the numbers more accurate with market value estimation

Data
 - Administrative property records (3 million), transaction records from ATTOM Data Solutions
 - Demographic data from census data
 - Flooding projections from National Oceanic and Atmospheric Administration (NOAA) 
 
Methodology
 - Build seperate models within each city and county 
 - Regularization models (Ridge, LASSO) as baseline
 - Regression trees e.g. random forest and gradient boosting as comparison for trial models

Findings
 - Trees work better than baseline
 - Random forest algorithm consistently outperformed by gradient boosting
 - City-level models have varying performance
 - Ability to predict within 10% deviation in some cities
 
---
## Project roadmap

1. Data preparation

2. Exploratory analysis

3. Seperate modeling: one model for each city/county data

4. Feature selection with individual models

5. Build seperate predictive models for 85 city-level subsamples

6. Functionalities to test and parse out performance metrics for regularized models and regression trees

7. Personalize data filtering parameters

8. Iteration through cities/counties and view corss-validated model performance, map county-level model performance

---
## 1. Data preparation
Select features from property records in a real estate broker's database. 
```{r, eval=FALSE}
#Input original dataset 1: property data
home_dta_original<-fread("raw_data/SF_Home_Characteristics.csv")
save(home_dta_original, file = "dta_processed/SF_Taxassessor_original.RData")
```

```{r, eval=FALSE, echo=TRUE}
#Select features to import, subset and save
home_dta <- select(
  home_dta_original,
  attomid,#Matching ID
  deedlastsaleprice,#Transaction price last sale
  situsstatecode,#State code
  situscounty,#County code
  propertyaddresscity,#City code
  ownertypedescription1,#First owner is individual/company?
  ownertypedescription2,#Second owner is individual/company?
  deedlastsaledate,#Date of market sale
  yearbuilt,#Year when built
  propertyusegroup,#Commercial/residential?
  areabuilding,#Living area in sq.feet
  censustract,#Census tract division
  propertylatitude,#Lat of property
  propertylongitude,#Lon of property
  roomsatticflag,#See below
  parkinggarage:communityrecroomflag#A series variable measuring physical attributes of the property, including rooms count and relevant facilities
)
```

```{r, eval=FALSE}
save(home_dta, file = "dta_processed/SF_Taxassessor_reduced.RData")
```

---
## 1. Data preparation
Select features from environmental risk and demographic data set constructed by Porter
```{r, eval=FALSE}
#Import dataset 2: property and environmental risk, subset and save
risk_dta_original <-
  fread("raw_data/SF_Parcel_Risk_and_Spatial_Data.csv")
save(risk_dta_original, file = "dta_processed/SF_Parcel_Risk_and_Spatial_Data_original.RData")
```

```{r, eval=FALSE, echo=TRUE}
risk_dta <-
  risk_dta_original %>% select(attomid = ATTOM_ID, #ID
                               dist_coast, #Distance to coast
                               mdkt32, #Flooding probability estimate in next years
                               Totpopbg:near_reading_rates, )#a set of demographic information varying by census tract
```

```{r, eval=FALSE}
save(risk_dta, file = "dta_processed/SF_Parcel_Risk_and_Spatial_Data_reduced.RData")
```

Get a simple feature of transaction frequence from transaction records
```{r, eval=FALSE}
#Import dataset 3: property transactions, subset and save
trans_dta_original <-
  fread("raw_data/SF_Sales_Transactions_Data.csv")
save(trans_dta_original, file = "dta_processed/SF_Sales_Transactions_Data_original.RData")
```

```{r, eval=FALSE, echo=TRUE}
trans_dta <-
  trans_dta_original %>% group_by(attomid) %>% summarize(trans_times = n())#Number of transactions
```

```{r, eval=FALSE}
save(trans_dta, file = "dta_processed/SF_Sales_Transactions_Data_reduced.RData")
```

```{r}
#Load and join saved RData
load("dta_processed/SF_Taxassessor_reduced.RData")
load("dta_processed/SF_Parcel_Risk_and_Spatial_Data_reduced.RData")
home_dta <- left_join(home_dta, risk_dta, by = "attomid")
load("dta_processed/SF_Sales_Transactions_Data_reduced.RData")
home_dta <- left_join(home_dta, trans_dta, by = "attomid")
```

```{r}
#Store (and report) a current version of data here
report.dim <- function(dta) {
  cat("Up to here, the data dimension is",
      dim(dta)[1],
      "*",
      dim(dta)[2])
}
home_dta_ver1 <- home_dta
home_dta_ver1 %>% report.dim()
```

---
## 1. Data preparation
```{r}
# - Recoding and transforming features
#This chunk of recoding is not supposed to change data dimension
#Fill in the price variable so that it will not be dropped later
home_dta<-home_dta%>%filter(is.na(home_dta$deedlastsaleprice) == "FALSE")
#Rounding deed last sale date to year and recoding NAs
home_dta <- home_dta %>% mutate(attomid=attomid %>% as.numeric(),
                                year_lastsale =
                                  str_sub(deedlastsaledate, start = 0, end = 4),
                                month_lastsale =
                                  str_sub(deedlastsaledate, start = 6, end = 7),
                                day_lastsale =
                                  deedlastsaledate%>%as.Date()%>%weekdays())
#Recoding ownership type misseallenous to NA
home_dta$ownertypedescription1[home_dta$ownertypedescription1 %in% c ("NP", "UNKNOWN", "") |
                                 is.na(home_dta$ownertypedescription1) == TRUE] <- 0
home_dta$ownertypedescription2[home_dta$ownertypedescription2 %in% c ("NP", "UNKNOWN", "") |
                                 is.na(home_dta$ownertypedescription2) == TRUE] <- 0
```

```{r}
home_dta[1:5000] %>% select(deedlastsaleprice,
  #Transaction price last sale
  situsstatecode,
  #State code
  situscounty,
  #County code
  propertyaddresscity,
  #City code
  ownertypedescription1,
  #First owner is individual/company?
  ownertypedescription2,
  #Second owner is individual/company?
  deedlastsaledate,
  #Date of market sale
  yearbuilt,
  #Year when built
  propertyusegroup,
  #Commercial/residential?
  areabuilding,
  #Living area in sq.feet
  censustract,
  #Census tract division
  propertylatitude,
  #Lat of property
  propertylongitude
) %>% missing_plot()
```

```{r}
#Recoding property usage group
home_dta$propertyusegroup[home_dta$propertyusegroup%in% c ("NP", "UNKNOWN", "") |
                                 is.na(home_dta$propertyusegroup) == TRUE] <- 0
#152 PropertyUseStandardized is better coded, should do this in future
#class_coding<-read.csv("D:/raw_data/prop_use_codes_trim.csv")
#Excluding <50 sq. feet living area
home_dta$areabuilding[home_dta$areabuilding<50]<-NA
```

 - Drop all unary features
```{r}
#Drop unary variables in dataset
drop.unary <- function(dta) {
  for (i in names(dta)[-1:-which(names(home_dta)=="propertylongitude")]) {
    var <- dta[[i]]
    var <- var[is.na(var) == FALSE]
    if ((var %>% unique() %>% length() %in% c(0, 1))) {
      dta <- dta %>% select(-i)
    }
  }
  return(dta)
}
home_dta <- home_dta %>% drop.unary()
home_dta_ver1.5 <- home_dta
home_dta_ver1.5 %>% report.dim()
```

 - Drop variables with too many levels, besides those are numerical
```{r}
#Drop variables with too many levels, besides those are numerical
for (i in names(home_dta)[which(names(home_dta) == "parkinggarage"):which(names(home_dta) ==
                                                                          "mdkt32")]) {
  if ((home_dta[[i]] %>% unique() %>% length() > 10) &
      (str_detect(i, pattern = "area") == FALSE) &
      (str_detect(i, pattern = "count") == FALSE) &
      (!i %in% c("dist_coast", "mdkt32", "trans_times"))) {
    print(i)
    home_dta <- home_dta %>% select(-i)
  }
}
#Store a stage version of data here
home_dta_ver2 <- home_dta
home_dta_ver2 %>% report.dim()
```
 
```{r}
#Give value 0 for NAs in less improtant variables so the obs will not be deleted
for (i in names(home_dta)[-1:-14]) {
  home_dta[[i]][is.na(home_dta[[i]]) == TRUE |
                  home_dta[[i]] == "" |
                  home_dta[[i]] == "NA"] <- 0
}
```
 
 - Recoded characters to factors
```{r}
#Recoded characters to factors.
cat("These variables are recoded to factors")
for (i in names(home_dta)) {
  if (class(home_dta[[i]]) == "character") {
    print(i)
    home_dta[[i]] <- home_dta[[i]] %>% as.factor()
  }
}
```

```{r}
#Get a list of large cities sufficient for consideration
city_obs <-
  group_by(home_dta, propertyaddresscity) %>% summarise(n())
names(city_obs) <- c("propertyaddresscity", "Properties")
```

 - Hold out cities with sample size too small to go into tree model.
```{r}
#For obtaining sufficient number of obs in each model, we drop cities with <5000 obs.
city_obs <- city_obs %>% filter(propertyaddresscity != "",
                                city_obs$Properties > 1000)
home_dta_less_city <-
  home_dta %>% filter(home_dta$propertyaddresscity %in% city_obs$propertyaddresscity)
home_dta <- home_dta_less_city
#Store a stage version of data here
home_dta_ver3 <- home_dta_less_city
home_dta_ver3 %>% report.dim()
save(home_dta,file = "dta_processed/home_dta.RData")
load("dta_processed/home_dta.RData")
```

---
exclude: true

## 2. Exploratory analysis
Plot summary graph of a few important variables to understand their overall statistical distribution
```{r}
#Plot summary of a few important variables
summary.variable <- function(varname = "deedlastsaleprice",
                             dta = home_dta) {
  #For numeric and integer variables
  if (class(dta[[varname]]) %in% c("numeric", "integer")) {
    m <- ggplot(data = dta) +
      geom_area(aes(x = dta[[varname]]),
                stat = "bin",
                fill = "khaki2") +
      xlab(paste(varname, class(dta[[varname]]))) +
      ylab("Properties") +
      theme_classic() +
      theme(axis.text.x = element_text(angle = 30, hjust = 1))+
  theme(text = element_text(size = 20))
  }
  #For factors and logicals
  if (class(dta[[varname]]) %in% c("factor", "logical")) {
    m <- ggplot(data = dta) +
      geom_bar(aes(x = dta[[varname]]),
               fill = "khaki2") +
      xlab(paste(varname, class(dta[[varname]]))) +
      ylab("Properties") +
      theme_classic() +
      theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
      theme(text = element_text(size = 20))
  }
  return(m)
}
```

```{r, eval=FALSE}
#Explorative function to plot summary statistics for a set of variables
var_plots <- lapply(colnames(home_dta), summary.variable)
grid_plot <- function(start_num = 1) {
  grid.arrange(
    var_plots[[start_num]],
    var_plots[[start_num + 1]],
    var_plots[[start_num + 2]],
    var_plots[[start_num + 3]],
    var_plots[[start_num + 4]],
    var_plots[[start_num + 5]],
    var_plots[[start_num + 6]],
    var_plots[[start_num + 7]],
    var_plots[[start_num + 8]],
    var_plots[[start_num + 9]],
    var_plots[[start_num + 10]],
    var_plots[[start_num + 11]],
    var_plots[[start_num + 12]],
    var_plots[[start_num + 13]],
    var_plots[[start_num + 14]],
    var_plots[[start_num + 15]],
    var_plots[[start_num + 16]],
    var_plots[[start_num + 17]],
    var_plots[[start_num + 18]],
    var_plots[[start_num + 19]]
  )
}
```

```{r, eval=FALSE}
grid_plot(1)
```

---
## 2. Exploratory analysis

 - Summarize a few key variables by county to see the geographical variation
```{r,include=FALSE}
#Import shape file and save to rdata to reduce loading time
parcel <-
  readOGR(dsn = "raw_data/tl_2017_us_county", layer = "tl_2017_us_county")
counties <-
  c("Broward",
    "Collier",
    "Hendry",
    "Lee",
    "Miami-Dade",
    "Monroe",
    "Palm Beach")
parcel <-
  parcel[parcel@data$NAME %in% counties & parcel@data$STATEFP == "12", ]
#Summary a few key variables by county
county_summary <-
  home_dta %>% group_by(situscounty) %>% summarize(
    Number_of_Properties = n() %>% as.numeric(),
    Average_Price = mean(deedlastsaleprice %>%
                           as.numeric(), na.rm = TRUE),
    Average_Built_Year = mean(yearbuilt %>%
                                as.numeric(), na.rm = TRUE),
    Average_Living_Area = mean(areabuilding %>%
                                 as.numeric(), na.rm = TRUE)
  )
names(county_summary)[1] <- c("NAME")
parcel@data <- inner_join(parcel@data, county_summary, by = "NAME")
save(parcel, file = "graph_data/parcel.RData")
```
 
```{r}
#Plot statistics by county
options(scipen = 10)
summary.county <- function(varname, par = parcel) {
  m <- tm_shape(parcel) +
    tm_fill(varname,
            palette = "GnBu") +
    tm_text("NAME", size = 0.6) +
    tm_borders(col = "black") +
    tm_layout(
      legend.title.size = 0.8,
      legend.text.size = 0.5,
      legend.position = c("left", "top"),
      legend.bg.color = "white",
      legend.bg.alpha = 0
    )
  return(m)
}
tmap_arrange(
  summary.county(varname = "Number_of_Properties"),
  summary.county(varname = "Average_Price"),
  summary.county(varname = "Average_Living_Area"),
  summary.county(varname = "Average_Built_Year"),
  nrow = 2
)
```

---
## 2. Exploratory analysis
```{r}
by_year_county <-
  home_dta %>% group_by(situscounty, deedlastsaledate) %>% summarize(
    num = n(),
    price =
      median(deedlastsaleprice, na.rm = TRUE),
    area = mean(areabuilding, na.rm =
                  TRUE),
    averageprice = mean(deedlastsaleprice /
                          areabuilding, na.rm = TRUE),
    builtyear = mean(yearbuilt, na.rm =
                       TRUE)
  )
#save(by_year_county,file = "graph_data/by_year_county.RData")
```
 
```{r}
ggplot(by_year_county[by_year_county$deedlastsaledate > 1955 &
                        by_year_county$deedlastsaledate < 2019 &
                        is.na(by_year_county$deedlastsaledate) == FALSE &
                        is.na(by_year_county$county) == FALSE, ],
       aes(x = deedlastsaledate,
           y = price,
           color = situscounty)) +
  scale_color_brewer(palette = "Set3") +
  geom_line(data = by_year_county[by_year_county$deedlastsaledate > 1955 &
                                    by_year_county$deedlastsaledate < 2019, ]) +
  geom_point(data = by_year_county[by_year_county$deedlastsaledate > 1955 &
                                     by_year_county$deedlastsaledate < 2019, ]) +
  scale_x_continuous(name = "Year of transaction",
                     breaks = c(seq(1955, 2018, by = 5), 2018)) +
  scale_y_continuous(name = "Average transaction price ($)",
                     breaks = seq(0, 2000000, by = 200000)) +
  ggtitle("Historical transaction price in each county") +
  labs(caption = "Source: ATTOM Data Solutions") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))+
  theme(text = element_text(size = 20))
```

---
## 2. Exploratory analysis
```{r}
by_year_use <-
  home_dta[home_dta$propertyusegroup != 0, ] %>% group_by(propertyusegroup, deedlastsaledate) %>%
  summarize(
    num = n(),
    price =
      median(deedlastsaleprice, na.rm = TRUE),
    area = mean(areabuilding, na.rm =
                  TRUE),
    averageprice = mean(deedlastsaleprice /
                          areabuilding, na.rm = TRUE),
    builtyear = mean(yearbuilt, na.rm =
                       TRUE)
  )
#save(by_year_use,file = "D:/Langyi/graph_data/by_year_use.RData")
```


```{r}
ggplot(by_year_use%>%filter(deedlastsaledate %in%c(1955:2019)),
       aes(x = deedlastsaledate,
           y = price,
           color = propertyusegroup)) +
  scale_color_brewer(palette = "Dark2") +
  geom_line() +
  geom_point() +
  scale_x_continuous(name = "Transaction year",
                     breaks = c(seq(1955, 2018, by = 5), 2018)) +
  scale_y_continuous(name = "Average property value ($)",
                     breaks = seq(0, 2000000, by = 200000)) +
  ggtitle("Historical price between commercial and residential properties") +
  labs(caption = "Source: ATTOM Data Solutions") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))+
  theme(text = element_text(size = 20))
```

---
## 2. Exploratory analysis
```{r}
by_year_own <-
  home_dta %>% filter(ownertypedescription1 != 0) %>% group_by(ownertypedescription1, deedlastsaledate) %>%
  summarize(
    num = n(),
    price =
      median(deedlastsaleprice, na.rm = TRUE),
    area = mean(areabuilding, na.rm =
                  TRUE),
    averageprice = mean(deedlastsaleprice /
                          areabuilding, na.rm = TRUE),
    builtyear = mean(yearbuilt, na.rm =
                       TRUE)
  )
#save(by_year_own,file = "D:/Langyi/graph_data/by_year_own.RData")
```
 
```{r}
ggplot(by_year_own%>%filter(deedlastsaledate %in%c(1955:2019)),
       aes(x = deedlastsaledate,
           y = price,
           color = ownertypedescription1)) +
  scale_color_brewer(palette = "Accent")+
  geom_line()+
  geom_point()+
  scale_x_continuous(name="Transaction year", 
                     breaks=c(seq(1955,2018,by=5),2018))+
  scale_y_continuous(name="Average property value ($)", 
                     breaks=seq(0,2000000,by=200000))+
  ggtitle("Historical price between company-ownerd and individual-owned properties")+
  labs(caption = "Source: ATTOM Data Solutions")+
  theme_classic()+
  theme(axis.text.x=element_text(angle=30, hjust=1))+
  theme(text = element_text(size = 20))
```

---
## 2. Exploratory analysis
```{r}
home_dta_flood <- home_dta
med_dist_coast <- median(home_dta_flood$dist_coast, na.rm = TRUE)
med_floodrisk <- median(home_dta_flood$mdkt32, na.rm = TRUE)
home_dta_flood$dist_coast1 <- NA
home_dta_flood$dist_coast1[home_dta_flood$dist_coast <= med_dist_coast] <-
  "no flood risk"
home_dta_flood$dist_coast1[home_dta_flood$dist_coast > med_dist_coast] <-
  "with flood risk"
home_dta_flood$dist_coast1 <-
  home_dta_flood$dist_coast1 %>% as.character()
home_dta_flood$floodrisk1 <- NA
home_dta_flood$floodrisk1[home_dta_flood$mdkt32 <= med_floodrisk] <-
  "no flood risk"
home_dta_flood$floodrisk1[home_dta_flood$mdkt32 > med_floodrisk] <-
  "with flood risk"
home_dta_flood$floodrisk1 <-
  home_dta_flood$floodrisk1 %>% as.character()
by_year_floodrisk <-
  home_dta_flood %>% group_by(floodrisk1, deedlastsaledate) %>% summarize(
    num = n(),
    price = median(deedlastsaleprice, na.rm = TRUE),
    area = mean(areabuilding, na.rm = TRUE),
    averageprice = mean(deedlastsaleprice / areabuilding, na.rm = TRUE),
    averagefrequency = mean(trans_times, na.rm = TRUE),
    builtyear = mean(yearbuilt, na.rm = TRUE)
  )
#save(by_year_floodrisk,file = "D:/Langyi/graph_data/by_year_floodrisk.RData")
```

```{r}
ggplot(
  by_year_floodrisk %>% filter(deedlastsaledate %in% c(1955:2019)),
  aes(x = deedlastsaledate,
      y = price,
      color = floodrisk1)
) +
  scale_color_brewer(palette = "Set2") +
  geom_line() +
  geom_point() +
  scale_x_continuous(name = "Year of transaction",
                     breaks = c(seq(2000, 2018, by = 5), 2018)) +
  scale_y_continuous(name = "Average transaction price ($)",
                     breaks = seq(0, 2000000, by = 200000)) +
  ggtitle("Average transaction price for risky assets and non-risky assets") +
  labs(caption = "Source: ATTOM Data Solutions") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))+
  theme(text = element_text(size = 20))
```

---
## 2. Exploratory analysis
```{r}
ggplot(by_year_floodrisk %>% filter(deedlastsaledate %in% c(2000:2019)),
       aes(x = deedlastsaledate,
           y = averagefrequency,
           color = floodrisk1)) +
  scale_color_brewer(palette = "Set2") +
  geom_line() +
  geom_point() +
  scale_x_continuous(name = "Year of transaction",
                     breaks = c(seq(2000, 2018, by = 5), 2018)) +
  scale_y_continuous(name = "Average transaction price ($)",
                     breaks = seq(0, 2000000, by = 200000)) +
  ggtitle("Average transaction frequency for risky assets and non-risky assets") +
  labs(caption = "Source: ATTOM Data Solutions") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))+
  theme(text = element_text(size = 20))
```

---
## 2. Exploratory analysis
```{r}
by_city <-
  home_dta_flood %>% group_by(propertyaddresscity) %>% summarize(
    num = n(),
    county = unique(situscounty)[1],
    price =
      median(deedlastsaleprice, na.rm = TRUE),
    area = median(areabuilding, na.rm =
                    TRUE),
    averageprice = median(deedlastsaleprice /
                            areabuilding, na.rm = TRUE),
    averagefrequency = mean(trans_times, na.rm =
                              TRUE),
    builtyear = median(yearbuilt, na.rm =
                         TRUE),
    floodrisk = mean(mdkt32, na.rm = TRUE)
  )
med_floodrisk <- median(by_city$floodrisk, na.rm = TRUE)
by_city$floodrisk1 <- NA
by_city$floodrisk1[by_city$floodrisk == 0] <- "no flood risk"
by_city$floodrisk1[by_city$floodrisk <= med_floodrisk &
                     by_city$floodrisk > 0] <- "low flood risk"
by_city$floodrisk1[by_city$floodrisk > med_floodrisk] <-
  "high flood risk"
by_city$floodrisk1 <- by_city$floodrisk1 %>% as.character()
#save(by_city,file = "D:/Langyi/graph_data/by_city.RData")
```

```{r, eval=FALSE}
by_city_year<-home_dta_flood%>%group_by(propertyaddresscity,deedlastsaledate)%>%summarize(num=n(),
                                                              county=unique(situscounty)[1],
                                                                                           price=median(deedlastsaleprice,na.rm=TRUE),
                                                  area=median(areabuilding,na.rm=TRUE),
                                              averageprice=median(deedlastsaleprice/areabuilding,na.rm=TRUE),
                                              averagefrequency=mean(trans_times,na.rm=TRUE),
                                              builtyear=median(yearbuilt,na.rm=TRUE),
                                              floodrisk=mean(mdkt32,na.rm = TRUE)
                                                  )
med_floodrisk<-median(by_city_year$floodrisk,na.rm = TRUE)
by_city_year$floodrisk1<-NA
by_city_year$floodrisk1[by_city_year$floodrisk==0]<-"no flood risk"
by_city_year$floodrisk1[by_city_year$floodrisk<=med_floodrisk&
                     by_city_year$floodrisk>0]<-"low flood risk"
by_city_year$floodrisk1[by_city_year$floodrisk>med_floodrisk]<-"high flood risk"
by_city_year$floodrisk1<-by_city_year$floodrisk1%>%as.character()
#save(by_city_year,file = "D:/Langyi/graph_data/by_city_year.RData")
```
 
```{r}
ggplot(by_city,
       aes(x = averagefrequency,
           y = price,
           size = num,
           color= floodrisk1)) +
  geom_point()+
  scale_color_brewer(palette = "OrRd", 
                     direction=-1)+
  scale_size(guide = "none")+
  scale_x_continuous(name="Average transaction times in market")+
  scale_y_continuous(name="Average property value ($)", 
                     breaks=seq(0,2000000,by=200000))+
  geom_text(data=by_city[by_city$num>15000,],
            aes(label=propertyaddresscity),
            position=position_jitter(width=1,height=1),
            color="black",
            size=2.5, hjust=-0.3, vjust=-0.3)+
  ggtitle("Average property price and transaction frequency in cities with different flooding risk",
          subtitle = "Large cities annotated")+
  labs(caption = "Source: ATTOM Data Solutions")+
  theme_classic()+
  theme(text = element_text(size = 20))
```

---
## 2. Exploratory analysis
```{r}
ggplot(by_city,
       aes(x = builtyear,
           y = price,
           size = num,
           color = floodrisk1)) +
  geom_point()+
  scale_size(guide = "none")+
  scale_color_brewer(palette = "OrRd", 
                     direction=-1)+
  scale_x_continuous(name="Year built of property", 
                     breaks=c(seq(1950,2018,by=5),2018))+
  scale_y_continuous(name="Average property value ($)", 
                     breaks=seq(0,2000000,by=200000))+
  geom_text(data=by_city[by_city$num>15000,],
            aes(label=propertyaddresscity),
            position=position_jitter(width=1,height=1),
            color= "black",size=2.5, hjust=-0.3, vjust=-0.3)+
  ggtitle("Average property price and year built in in cities with different flooding risk",
          subtitle = "Large cities annotated")+
  labs(caption = "Source: ATTOM Data Solutions")+
  theme_classic()+
  theme(axis.text.x=element_text(angle=30, hjust=1))+
  theme(text = element_text(size = 20))
```



---
## 3. Seperate modeling: one model for each city/county data
 - For every city/county in South Florida, we fit an individual model to it seperately.

 - Inspired by the "submarket" notion (the housing prices between neibourhoods, cities and counties vary a lot)
 
 - Feature structure vary by county

 - There are some variables that represent housing attributes that miss in data with different cases between counties.

 - The process for county-level modeling is rather straightforward.
 
```{r}
home_dta_less_city$propertyaddresscity <-
  home_dta_less_city$propertyaddresscity %>% as.character()
#Split data at city level
home_city_dta <-
  split(home_dta_less_city, home_dta_less_city$propertyaddresscity)
#Split data at county level, get rid of census tract
home_county_dta <-
  split(select(home_dta, -censustract), home_dta$situscounty)
```

---
## 4. Feature selection with individual models
 - Build function to automatically drop variables that are not important and are missing over 10% values, as we don't wish too many obs are omitted in tree model due to missing value.
```{r}
#Function to determine whether a variable is missing less than 10% values
is.missing <- function(var, ratio = 0.1) {
  a <- var %>% length()
  b <- var %>% is.na() %>% sum()
  b / a < ratio
}
#Function to preprocess a data frame and drop according to is.missing
#Here some features are kept due to their apparent relevancy which does not worth being dropped due to missing values
drop.missing <- function(dta) {
  varnames_important <- names(dta)[1:14]
  varnames_supplement <- names(dta)[-1:-14]
  for (i in varnames_important) {
    var <- dta[[i]]
    if (is.missing(var, ratio = 0.5) == "FALSE") {
      dta <- select(dta, -i)
    }
  }
  for (i in varnames_supplement) {
    var <- dta[[i]]
    if (is.missing(var, ratio = 0.1) == "FALSE") {
      dta <- select(dta, -i)
    }
  }
  return(dta)
}
```

 - Apply the selection functions built just now to all subsamples
```{r}
#Apply to city the function in previous function for feature selection
home_city_cleaned_dta <- purrr::map(home_city_dta, drop.missing)
home_city_cleaned_dta <- purrr::map(home_city_cleaned_dta, drop.unary)
#Apply to county the function in previous function for feature selection
home_county_cleaned_dta <- purrr::map(home_county_dta, drop.missing)
home_county_cleaned_dta <- purrr::map(home_county_cleaned_dta, drop.unary)
```

 - Divided subsamples by county
```{r, echo=TRUE}
summary(home_county_cleaned_dta)
```
---
## 4. Feature selection with individual models
 - First 5 divided subsamples by city
```{r, echo=TRUE}
summary(home_city_cleaned_dta)%>%head()
```

---
exclude: true
## 5. Interactivity to check individual properties within sample (a playaround...)
 - Build the ability to check for key features of individual properties using Google map
```{r}
#Provide an interactive leaflet for checking individual properties, Hendry County as an example.
map.variable<-function(dta=home_county_cleaned_dta$Hendry,
                       varname="propertyusegroup"){
if (varname%in%names(dta)==TRUE&
    "propertylatitude"%in%names(dta)==TRUE&
    "propertylongitude"%in%names(dta)==TRUE){
  dta<-dta[is.na(dta[[varname]])==FALSE,]
library(leaflet)
pal = colorFactor("Set1", dta[[varname]])
color_offsel1 = pal(dta[[varname]])
leaflet(dta) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~propertylongitude, 
             lat = ~propertylatitude,
             color = color_offsel1,
             popup = content<<-str_c(varname,
                                     dta[[varname]],
                                     "Market price",
                                     dta$deedlastsaleprice,
                                     "Transaction date",
                                     dta$deedlastsaledate,
                                     "Living area",
                                     dta$areabuilding,
                                     sep = " "),
             clusterOptions = markerClusterOptions())%>%
  addLegend(pal = pal, values = ~dta[[varname]], title = varname)
} else {print("Variables filtered out in final data, unable to plot map for this subsample")
}
}
``` 
 
```{r, eval=FALSE}
map.variable(dta = home_city_cleaned_dta$AVENTURA)
```

---
## 6. Build predictive models for individual subgroups
###Final Ppreprocessing of individual model before modeling
 - Develop functionality for final preprocessing before feeding data into model

 - Here we have within each model the control of missing values and division of training/test set

 - Therefore, the wragling process was automized and run in each subsample
 
```{r}
#Build final preprocess function
model.preprocess <- function(dta,
                             price_high,
                             price_low,
                             start_year,
                             size) {
  #Delete lng/lat info used for geomapping
  dta$propertylatitude<-NULL
  dta$propertylongitude<-NULL
  #Recode tract to factor. If there are more than 32 tracts, exclude tracts.
  if ("censustract" %in% names(dta)) {
    if (dta$censustract %>% unique() %>% length() < 32) {
      dta$censustract <- dta$censustract %>% as.factor()
    } else {
      dta <- dta %>% select(-censustract)
    }
  }
  dta <-
    filter(dta, is.na(dta$deedlastsaleprice) == FALSE)#Exclude NA in y variable
  row_before <- nrow(dta)
  #Excluding high value transactions to suppress extreme values for convenience
  dta$deedlastsaleprice[dta$deedlastsaleprice >= price_high] <- NA
  #Excluding low value transactions which are not authentic
  dta$deedlastsaleprice[dta$deedlastsaleprice <= price_low] <- NA
  dta$deedlastsaleprice <- dta$deedlastsaleprice %>% as.numeric()
  #Exclude NA in y variable again
  dta <- filter(dta, is.na(dta$deedlastsaleprice) == FALSE)
  row_after <- nrow(dta)
  retention <<- row_after / row_before
  #Pick a start year for modeling
  if ("deedlastsaledate" %in% names(dta))
    dta <- dta %>% filter(deedlastsaledate >= start_year)
  #Option to reduce sample size for computation convenience
  set.seed(0)
  if (nrow(dta) >= size) {
    dta <- sample_n(dta, size)
  }
  dta <- drop.unary(dta)
  return(dta)
}
```

###Modeling functionalities
 - Regularization (Ridge and LASSO) as baseline, try also trees
 
 - Random forest deal with data with large variations well
 
 - Gradient boosting is good at learn specific models 
 
 - A model comparison with output from random forest and gradient boosting (also Ridge and LASSO)
 
 - The function will parse validated key performance metrics and variables of importance of the models

```{r}
model.comparison.tree <-
  function(dta, preprocess_retention = retention) {
    #Split training/test samples (0.7:0.3)
    train <- sample_frac(dta, size = 0.7)
    test <- anti_join(dta, train, by = "attomid")
    #Prepare y and x features
    train <-
      select(train,
             -attomid,
             -situsstatecode,
             -situscounty,
             -propertyaddresscity)
    y <- train$deedlastsaleprice
    x <- select(train, -deedlastsaleprice)
    #Try random forest
    library(randomForest)
    train_rf <- randomForest(y ~ ., x,
                             importance = TRUE,
                             na.action = "na.omit")
    rf_summary <- importance(train_rf) %>% as.data.frame()
    rf_highvars <- rf_summary[order(rf_summary$`%IncMSE`,
                                    decreasing = TRUE),] %>% head(5) %>% row.names()
    if (length(rf_highvars) < 5) {
      rf_highvars <- c(rf_highvars, rep("N/A", (5 - length(rf_highvars))))
    }
    #Test set
    yhat.rf <- predict(train_rf, test)
    rf_MSE <-
      mean((yhat.rf - test$deedlastsaleprice) ^ 2, na.rm = TRUE)
    rf_MPE <-
      mean((yhat.rf - test$deedlastsaleprice) / test$deedlastsaleprice,
           na.rm = TRUE)
    rf_MAPE <-
      mean(abs((yhat.rf - test$deedlastsaleprice) / test$deedlastsaleprice), na.rm = TRUE)
    #Boosting
    library(gbm)
    train_gb <- gbm(y ~ ., x,
                    n.trees = 1000,
                    distribution = "gaussian")
    gb_summary <- summary(train_gb, plotit = FALSE)
    gb_highvars <- gb_summary[order(gb_summary$rel.inf,
                                    decreasing = TRUE),] %>% head(5) %>% row.names()
    if (length(gb_highvars) < 5) {
      add <- 5 - length(gb_highvars)
      gb_highvars <- c(gb_highvars, rep("N/A", add))
    }
    #Test set
    yhat.gb <- predict(train_gb, test, n.trees = 1000)
    gb_MSE <-
      mean((yhat.gb - test$deedlastsaleprice) ^ 2, na.rm = TRUE)
    gb_MPE <-
      mean((yhat.gb - test$deedlastsaleprice) / test$deedlastsaleprice,
           na.rm = TRUE)
    gb_MAPE <-
      mean(abs((yhat.gb - test$deedlastsaleprice) / test$deedlastsaleprice), na.rm = TRUE)
    #Store comparison between rf and gb
    if (dta$propertyaddresscity %>% unique() %>% length() == 1) {
      subsample_name <-
        paste(
          dta$situsstatecode %>% unique() %>% as.character() %>% paste(collapse = "_"),
          dta$situscounty %>% unique() %>% as.character() %>% paste(collapse = "_"),
          dta$propertyaddresscity %>% unique() %>% as.character() %>% paste(collapse = "_"),
          sep = "_"
        )
    } else {
      subsample_name <-
        paste(
          dta$situsstatecode %>% unique() %>% as.character() %>% paste(collapse =
                                                                         "_"),
          dta$situscounty %>% unique() %>% as.character() %>%
            paste(collapse = "_")
        )
      
    }
    comparison <- c(
      subsample_name,
      preprocess_retention,
      nrow(dta),
      ncol(dta),
      rf_MSE,
      rf_MPE,
      rf_MAPE,
      rf_highvars,
      gb_MSE,
      gb_MPE,
      gb_MAPE,
      gb_highvars
    ) %>% as.data.frame %>% t()
    colnames(comparison) <- c(
      "subsample",
      "preprocess_retention",
      "obs",
      "vars",
      "rf_MSE",
      "rf_MPE",
      "rf_MAPE",
      "rf_1st",
      "rf_2nd",
      "rf_3rd",
      "rf_4th",
      "rf_5th",
      "gb_MSE",
      "gb_MPE",
      "gb_MAPE",
      "gb_1st",
      "gb_2nd",
      "gb_3rd",
      "gb_4th",
      "gb_5th"
    )
    return(comparison)
  }
```

```{r}
model.comparison.regularization <-
  function(dta, preprocess_retention = retention) {
    preprocess_retention <- retention
    #Split training/test samples (0.7:0.3)
    train <- sample_frac(dta, size = 0.7)
    test <- anti_join(dta, train, by = "attomid")
    #Prepare y and x features in train
    train <- train %>% select(-attomid,-situsstatecode,-situscounty,-propertyaddresscity)
    essential.var <- intersect(names(dta),
                               c(
                                 "deedlastsaledate",
                                 "yearbuilt",
                                 "areabuilding",
                                 "censustract"
                               )) %>% paste(collapse = "+")
    if ("deedlastsaledate" %in% names(dta)) {
      formula.1 <- as.formula(
        paste(
          "deedlastsaleprice~
                   I(deedlastsaledate^5)+
                  I(deedlastsaledate^4)+
                  I(deedlastsaledate^3)+
                  I(deedlastsaledate^2)+(",
          paste(essential.var),
          ")^2+
                    (.)"
        )
      )
    } else{
      formula.1 <-
        as.formula(paste("deedlastsaleprice~(", paste(essential.var), ")^2+(.)"))
    }
    train <- model.frame(formula.1,
                         data = train,
                         na.action = na.omit)
    y.train <- train$deedlastsaleprice
    x.train <- model.matrix(formula.1,
                            data = train)
    #Prepare test
    test <- test %>% select(-attomid,-situsstatecode,-situscounty,-propertyaddresscity)
    test <- model.frame(formula.1,
                        data = test,
                        na.action = na.omit)
    y.test <- test$deedlastsaleprice
    x.test <- model.matrix(formula.1,
                           data = test)
    #Predict with Ridge
    cv.out <- cv.glmnet(x.train, y.train, alpha = 0)
    bestlam <- cv.out$lambda.min
    out <-
      glmnet(
        x.train,
        y.train,
        nlambda = 1,
        lambda = bestlam,
        alpha = 0
      )
    yhat.ridge <- predict(cv.out, s = bestlam, newx <- x.test)
    ridge_MSE <- mean((yhat.ridge - y.test) ^ 2, na.rm = TRUE)
    ridge_MPE <- mean((yhat.ridge - y.test) / y.test, na.rm = TRUE)
    ridge_MAPE <- mean(abs((yhat.ridge - y.test) / y.test), na.rm = TRUE)
    #Predict with LASSO
    cv.out <- cv.glmnet(x.train, y.train, alpha = 1)
    bestlam <- cv.out$lambda.min
    out <-
      glmnet(
        x.train,
        y.train,
        nlambda = 1,
        lambda = bestlam,
        alpha = 1
      )
    yhat.LASSO <- predict(cv.out, s = bestlam, newx <- x.test)
    LASSO_MSE <- mean((yhat.LASSO - y.test) ^ 2, na.rm = TRUE)
    LASSO_MPE <- mean((yhat.LASSO - y.test) / y.test, na.rm = TRUE)
    LASSO_MAPE <- mean(abs((yhat.LASSO - y.test) / y.test), na.rm = TRUE)
    #Export important variables
    LASSO_coef <-
      coef(out) %>% row.names() %>% cbind(coef(out) %>% as.numeric())
    LASSO_coef <- LASSO_coef %>% as.data.frame()
    names(LASSO_coef) <- c("var", "coef")
    LASSO_coef$coef <- LASSO_coef$coef %>% as.character() %>% as.numeric()
    LASSO_coef$var <- LASSO_coef$var %>% as.character()
    LASSO_coef$abs_coef <- abs(LASSO_coef$coef)
    LASSO_coef <-
      LASSO_coef[order(LASSO_coef$abs_coef, decreasing = TRUE), ]
    LASSO_vars <- LASSO_coef$var[LASSO_coef$coef != 0 &
                                   LASSO_coef$var != "(Intercept)"]
    LASSO_length <- LASSO_vars %>% length()
    LASSO_names <- LASSO_vars %>% paste(collapse = "  |  ")
    #Store comparison between Ridge and LASSO
    if (dta$propertyaddresscity %>% unique() %>% length() == 1) {
      subsample_name <-
        paste(
          dta$situsstatecode %>% unique() %>% as.character() %>% paste(collapse =
                                                                         "_"),
          dta$situscounty %>% unique() %>% as.character() %>%
            paste(collapse = "_"),
          dta$propertyaddresscity %>% unique() %>% as.character() %>%
            paste(collapse = "_"),
          sep = "_"
        )
    } else {
      subsample_name <-
        paste(
          dta$situsstatecode %>% unique() %>% as.character() %>% paste(collapse =
                                                                         "_"),
          dta$situscounty %>% unique() %>% as.character() %>%
            paste(collapse = "_")
        )
      
    }
    comparison <- c(
      subsample_name,
      preprocess_retention,
      nrow(dta),
      ncol(dta),
      ridge_MSE,
      ridge_MPE,
      ridge_MAPE,
      LASSO_MSE,
      LASSO_MPE,
      LASSO_MAPE,
      LASSO_length,
      LASSO_names
    ) %>% as.data.frame %>% t()
    colnames(comparison) <- c(
      "subsample",
      "preprocess_retention",
      "obs",
      "vars",
      "ridge_MSE",
      "ridge_MPE",
      "ridge_MAPE",
      "LASSO_MSE",
      "LASSO_MPE",
      "LASSO_MAPE",
      "LASSO_length",
      "LASSO_names"
    )
    return(comparison)
  }
```
 
```{r}
#Assembling the final modeling function combining the previous 2 functions
#The function is applicable to large list.
#Build function to parse modeling output and realize parameter entry
modeling <- function(list,
                     method = "regularization",
                     price_h = 5000000,
                     price_l = 100,
                     yr_st = 2012,
                     sz = 1000) {
  comparison_set <- NULL
  for (i in 1:length(list)) {
    dta <- list[[i]]
    dta_preprocessed <- model.preprocess(
      dta,
      price_high = price_h,
      price_low = price_l,
      start_year = yr_st,
      size = sz
    )
    if (method == "tree") {
      comparison <- model.comparison.tree(dta_preprocessed)
    }
    if (method == "regularization") {
      comparison <- model.comparison.regularization(dta_preprocessed)
    }
    comparison_set <- rbind(comparison_set, comparison)
  }
  comparison_set <- cbind(
    rep(price_h, nrow(comparison_set)),
    rep(price_l, nrow(comparison_set)),
    rep(yr_st, nrow(comparison_set)),
    rep(sz, nrow(comparison_set)),
    comparison_set
  )
  comparison_set <- comparison_set %>% as.data.frame()
  names(comparison_set)[1:4] <-
    c("price_high", "price_low", "year_start", "size")
  comparison_set$preprocess_retention <-
    comparison_set$preprocess_retention %>% as.character() %>% as.numeric()
  num_vars <- c(
    "obs",
    "vars",
    "rf_MSE",
    "rf_MAPE",
    "gb_MSE",
    "gb_MPE",
    "gb_MAPE",
    "ridge_MSE",
    "ridge_MPE",
    "ridge_MAPE",
    "LASSO_MSE",
    "LASSO_MPE",
    "LASSO_MAPE",
    "LASSO_length"
  )
  for (i in names(comparison_set)) {
    if (i %in% num_vars) {
      comparison_set[[i]] <-
        comparison_set[[i]] %>% as.character() %>% as.numeric()
    }
  }
  comparison_set<-comparison_set%>%as.data.frame()
  return(comparison_set)
}
```

---
exclude: true
## 7. Test between 4 algorithms
Test the model in subsample of cities and counties, compare several KPI of models to see which of random forest and gradient boosting does the job better.
```{r, eval=FALSE}
model_city_tree <- modeling(home_city_cleaned_dta, "tree", sz = 300)
model_city_regularization <-
  modeling(home_city_cleaned_dta, "regularization", sz = 10000)
model_summary <- inner_join(
  model_city_tree,
  model_city_regularization %>% select(subsample,
                                       ridge_MSE:LASSO_names)
) 
save(model_summary, file = "dta_processed/model_summary.RData")
```

---
## 7 Visualize model performance
```{r}
load("dta_processed/model_summary.RData")
city_stats <-
  home_dta %>% group_by(propertyaddresscity) %>% summarise(
    num = n(),
    subsample =
      str_c(situsstatecode[1],
            situscounty[1],
            propertyaddresscity[1],
            sep = "_"),
    price = median(deedlastsaleprice, na.rm = TRUE),
    area = median(areabuilding, na.rm = TRUE),
    averageprice = median(deedlastsaleprice / areabuilding, na.rm = TRUE),
    builtyear = median(yearbuilt, na.rm = TRUE),
    dist_coast = mean(dist_coast, na.rm = TRUE),
    floodrisk = mean(mdkt32, na.rm = TRUE)
  )
major_models <- city_stats %>% inner_join(model_summary, by = "subsample")
major_models$rf_MPE <-
  major_models$rf_MPE %>% as.character() %>% as.numeric()
med_floodrisk1 <- median(major_models$floodrisk, na.rm = TRUE)
major_models$floodrisk1 <- NA
major_models$floodrisk1[major_models$floodrisk == 0] <- "no flood risk"
major_models$floodrisk1[major_models$floodrisk > 0 &
                          major_models$floodrisk <= med_floodrisk1] <-
  "low flood risk"
major_models$floodrisk1[major_models$floodrisk > med_floodrisk1] <-
  "high flood risk"
#save(major_models,file = "D:/Langyi/graph_data/major_models.RData")
```

```{r}
dta<-major_models%>%filter(gb_MAPE<1,rf_MAPE<1)
ggplot(dta)+
  geom_point(aes(x=propertyaddresscity[order(dta$rf_MAPE)],
                 y=rf_MAPE,
                 color="chartreuse"))+
  geom_point(aes(x=propertyaddresscity[order(dta$rf_MAPE)],
                 y=gb_MAPE,
                 color="aquamarine2"))+
  ggtitle("Random forest mostly outperform gradient boosting",
          subtitle = "Showing models with MAPE < 1, gradient boosting in blue") +
  xlab("City-level subsamples") + 
  ylab("MAPE")+
  labs(caption = "Source: ATTOM Data Solutions",
       fill = "Model") +
  theme_classic() +
  theme(axis.text.x=element_blank()+
  theme(text = element_text(size = 20)),
        legend.position = "none")
```

---
## 7 Visualize model performance
```{r}
dta<-major_models%>%filter(ridge_MAPE<1,LASSO_MAPE<1)
ggplot(dta)+
  geom_point(aes(x=propertyaddresscity[order(dta$rf_MAPE)],
                 y=LASSO_MAPE,
                 color="chartreuse"))+
  geom_point(aes(x=propertyaddresscity[order(dta$rf_MAPE)],
                 y=ridge_MAPE,
                 color="aquamarine2"))+
  ggtitle("Ridge and LASSO have similar performance",
          subtitle = "Showing models with MAPE < 1, ridge in blue") +
  xlab("City-level subsamples") + 
  ylab("MAPE")+
  labs(caption = "Source: ATTOM Data Solutions",
       fill = "Model") +
  theme_classic() +
  theme(axis.text.x=element_blank()+
  theme(text = element_text(size = 20)),
        legend.position = "none")
```

---
## 7 Visualize model performance
```{r}
dta<-major_models%>%filter(ridge_MAPE<1,gb_MAPE<1)
ggplot(dta)+
  geom_point(aes(x=propertyaddresscity[order(dta$rf_MAPE)],
                 y=gb_MAPE,
                 color="chartreuse"))+
  geom_point(aes(x=propertyaddresscity[order(dta$rf_MAPE)],
                 y=ridge_MAPE,
                 color="aquamarine2"))+
  ggtitle("Disparity exists for comparison of Ridge and GBM performance",
          subtitle = "Showing models with MAPE < 1, ridge in blue") +
  xlab("City-level subsamples") + 
  ylab("MAPE")+
  labs(caption = "Source: ATTOM Data Solutions",
       fill = "Model") +
  theme_classic() +
  theme(axis.text.x=element_blank()+
  theme(text = element_text(size = 20)),
        legend.position = "none")
```

---
## 7 Visualize model performance
```{r,fig.width=15}
major_models%>%select(subsample,
                      averageprice,
                      floodrisk,
                      rf_MAPE,
                      gb_MAPE,
                      ridge_MAPE,
                      LASSO_MAPE)%>%DT::datatable()
```

---
## 7.5 Visualize model performance
```{r}
ggplot(major_models,
       aes(
         x = price,
         y = floodrisk,
         color = floodrisk1,
         size = num,
         shape = rf_1st
       )) +
  geom_point() +
  geom_jitter() +
  scale_size(guide = "none")+
  scale_color_brewer(palette = "OrRd",
                     direction = -1) +
  scale_x_continuous(name = "Average property value ($)",
                     breaks = seq(0, 2000000, by = 200000)) +
  scale_y_continuous(name = "Level of flood risk") +
  geom_text(
    data = major_models%>%filter(rf_MPE < 0.3,
                                 floodrisk > 0.03),
    aes(label = propertyaddresscity),
    color = "black",
    size = 2.5,
    hjust = -0.3,
    vjust = -0.3
  ) +
  ggtitle("Primary factor of property value in cities",
          subtitle = "Cities with high flood risk and high prediction accuracy (within 30%) annotated") +
  labs(caption = "Source: ATTOM Data Solutions") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))+
  theme(text = element_text(size = 20))
```


---
## 8. Personalize filtering parameters for random forest
 - Filtering rule decides whether to include properties with extremely low/high price
 - Cruicial to model performance
 - Build functionalities to show model performance depending on filtering parameters
```{r}
#Build function to plot series of error for a given tuning parameter
error.plot <- function(dta = error_matrix,
                       title = "Parameter values") {
  retention <- ggplot(data = dta) +
    geom_line(aes(x = Parameter*100,
                  y = Retention),
              color = "tomato4") +
    xlab(title) +
    ylab("% of obs kept") + 
    theme(legend.position = "left") +
    theme_classic()+
  theme(text = element_text(size = 20))
  mse <- ggplot(data = dta) +
    geom_line(aes(x = Parameter,
                  y = MSE),
              color = "tomato4") +
    xlab(title) +
    ylab("MSE") + 
    theme(legend.position = "left") +
    theme_classic()+
  theme(text = element_text(size = 20))
  mpe <- ggplot(data = dta) +
    geom_line(aes(x = Parameter*100,
                  y = MPE),
              color = "tomato4") +
    xlab(title) +
    ylab("MPE %") + 
    theme(legend.position = "left") +
    theme_classic()+
  theme(text = element_text(size = 20))
  mape <- ggplot(data = dta) +
    geom_line(aes(x = Parameter*100,
                  y = MAPE),
              color = "tomato4") +
    xlab(title) +
    ylab("MAPE %") + 
    theme(legend.position = "left") +
    theme_classic()+
  theme(text = element_text(size = 20))
  grid.arrange(retention, mse, mape, mpe)
}
```
---
## 8. Personalize filtering parameters for random forest
```{r, eval=FALSE}
error_matrix<-NULL
for (i in seq(100,1000,100)){
output<-modeling(home_county_cleaned_dta,
                 method="tree",
                      price_h=5000000,
                      price_l=100,
                      sz=i,
                      yr_st=2012)
error_performance<-cbind(i,
                         output$preprocess_retention%>%mean(),
                         output$rf_MSE%>%mean(),
                         output$rf_MPE%>%mean(),
                         output$rf_MAPE%>%mean()
)
error_matrix<-rbind(error_matrix,error_performance)
}
colnames(error_matrix)<-c("Parameter","Retention","MSE","MPE","MAPE")
error_matrix<-error_matrix%>%as.data.frame()
save(error_matrix,file = "dta_processed/sz.RData")
```
 
```{r}
load("dta_processed/sz.RData")
error.plot(error_matrix, "Maximum sample size")
```

---
## 8. Personalize filtering parameters for random forest 
```{r, eval=FALSE}
error_matrix<-NULL
for (i in seq(100,5000,400)){
output<-modeling(home_county_cleaned_dta,
                 method="tree",
                      price_h=5000000,
                      price_l=i,
                      sz=500, 
                      yr_st=2012)
error_performance<-cbind(i,
                         output$preprocess_retention%>%mean(),
                         output$rf_MSE%>%mean(),
                         output$rf_MPE%>%mean(),
                         output$rf_MAPE%>%mean()
)
error_matrix<-rbind(error_matrix,error_performance)
}
colnames(error_matrix)<-c("Parameter","Retention","MSE","MPE","MAPE")
error_matrix<-error_matrix%>%as.data.frame()
save(error_matrix,file = "dta_processed/price_l.RData")
```
 
```{r}
load("dta_processed/price_l.RData")
error.plot(error_matrix, "Lowest price of property to be included")
```

---
## 8. Personalize filtering parameters for random forest 
```{r, eval=FALSE}
error_matrix<-NULL
for (i in c(2000:2017)){
output<-modeling(home_county_cleaned_dta,
                 method="tree",
                      price_h=5000000,
                      price_l=100,
                      sz=500, 
                      yr_st=i)
error_performance<-cbind(i,
                         output$preprocess_retention%>%mean(),
                         output$rf_MSE%>%mean(),
                         output$rf_MPE%>%mean(),
                         output$rf_MAPE%>%mean()
)
error_matrix<-rbind(error_matrix,error_performance)
}
colnames(error_matrix)<-c("Parameter","Retention","MSE","MPE","MAPE")
error_matrix<-error_matrix%>%as.data.frame()
save(error_matrix,file = "dta_processed/yr_st.RData")
```
 
```{r}
load("dta_processed/yr_st.RData")
error.plot(error_matrix, "Oldest property record to be included")
```

---
## 8. Personalize filtering parameters for random forest 
```{r, eval=FALSE}
error_matrix<-NULL
for (i in seq(100000,10000000,400000)){
output<-modeling(home_county_cleaned_dta,
                 method="tree",
                      price_h=i,
                      price_l=100,
                      sz=500, 
                      yr_st=2012)
error_performance<-cbind(i,
                         output$preprocess_retention%>%mean(),
                         output$rf_MSE%>%mean(),
                         output$rf_MPE%>%mean(),
                         output$rf_MAPE%>%mean()
)
error_matrix<-rbind(error_matrix,error_performance)
}
colnames(error_matrix)<-c("Parameter","Retention","MSE","MPE","MAPE")
error_matrix<-error_matrix%>%as.data.frame()
save(error_matrix,file = "dta_processed/price_h.RData")
```
 
```{r}
load("dta_processed/price_h.RData")
error.plot(error_matrix, "Highest price of property to be included")
```

---
## 9. Iteration through cities/counties and view corss-validated model performance
 - Cities
```{r,eval=FALSE}
rf_set_city<-modeling(home_city_cleaned_dta,
                      method = "regularization")
save(rf_set_city,file = "dta_processed/regularization_rf_set_city.RData")
```

```{r,fig.width=15}
load("dta_processed/regularization_rf_set_city.RData")
rf_set_city[order(rf_set_city$LASSO_MAPE), ] %>% select(subsample,
                                                        ridge_MPE,
                                                        ridge_MAPE,
                                                        LASSO_MPE,
                                                        LASSO_MAPE) %>% DT::datatable()
```


---
## 9. Iteration through cities/counties and view corss-validated model performance
 - Counties
```{r, eval=FALSE}
rf_set_county<-modeling(home_county_cleaned_dta,
                       method = "regularization")
save(rf_set_county,file = "dta_processed/regularization_rf_set_county.RData")
```

```{r,fig.width=20}
load("dta_processed/regularization_rf_set_county.RData")
rf_set_county[order(rf_set_county$LASSO_MAPE),] %>% select(subsample,
                                                           ridge_MPE,
                                                           ridge_MAPE,
                                                           LASSO_MPE,
                                                           LASSO_MAPE) %>% DT::datatable()
```

---
## 10. Map county-level model performance
We can to see how each county performs on the map.
```{r}
#merge model performance data by county to parcel file
rf_set_county1 <- rf_set_county
rf_set_county1$subsample <-
  rf_set_county1$subsample %>% str_sub(start = 4L)
names(rf_set_county1)[5] <- "NAME"
parcel@data <- inner_join(parcel@data, rf_set_county1, by = "NAME")
``` 

```{r}
tmap_arrange(
  summary.county(varname = "preprocess_retention"),
  summary.county(varname = "obs"),
  summary.county(varname = "vars"),
  summary.county(varname = "LASSO_MSE"),
  summary.county(varname = "LASSO_MPE"),
  summary.county(varname = "LASSO_MAPE"),
  ncol = 3
)
```

---
exclude: true
## 10. Map county-level model performance
If we recall previous exploratory analysis, the county's performance relies quite some on the features of each individual county. For some counties which lack a considerable number of important attributes, the prediction power is small. This sugggests it's correct to model by county to avoid the data quality issue of some counties will spread to the whole sample, but how to improve the low data quality prediction can be a subsequent challenge.

---
## 11. Brief Summary

 - Random forest algorithm consistently outperformed by gradient boosting

 - City-level models have varying performance and have potential for better model selection

 - Ability to predict within 10% deviation in some cities

 - Key filtering parameters optimized to improve prediction with reasonable constraint on sample size

 - Transaction time and building area is 2 primary factor to determine property value
 
## 12. Next step

 - Add census tract block medium price as control

 - Can try tuning the filtering parameter for each model

 - log scale price

 - recode zip code to dummy (one-hot recoding)